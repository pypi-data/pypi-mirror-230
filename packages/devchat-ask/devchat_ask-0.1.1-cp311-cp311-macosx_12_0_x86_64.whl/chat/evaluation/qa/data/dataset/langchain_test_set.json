{
    "name": "LangchainSet",
    "description": "Test suite on the langchain project.",
    "cases": [
        {
            "id": "langchain-1",
            "question": "What is the class hierarchy?",
            "reference_answer": "There are several class hierarchies in the provided code, so I'll list a few:\n\nBaseModel -> ConstitutionalPrinciple: ConstitutionalPrinciple is a subclass of BaseModel.\nBasePromptTemplate -> StringPromptTemplate, AIMessagePromptTemplate, BaseChatPromptTemplate, ChatMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, FewShotPromptTemplate, FewShotPromptWithTemplates, Prompt, PromptTemplate: All of these classes are subclasses of BasePromptTemplate.\nAPIChain, Chain, MapReduceDocumentsChain, MapRerankDocumentsChain, RefineDocumentsChain, StuffDocumentsChain, HypotheticalDocumentEmbedder, LLMChain, LLMBashChain, LLMCheckerChain, LLMMathChain, LLMRequestsChain, PALChain, QAWithSourcesChain, VectorDBQAWithSourcesChain, VectorDBQA, SQLDatabaseChain: All of these classes are subclasses of Chain.\nBaseLoader: BaseLoader is a subclass of ABC.\nBaseTracer -> ChainRun, LLMRun, SharedTracer, ToolRun, Tracer, TracerException, TracerSession: All of these classes are subclasses of BaseTracer.\nOpenAIEmbeddings, HuggingFaceEmbeddings, CohereEmbeddings, JinaEmbeddings, LlamaCppEmbeddings, HuggingFaceHubEmbeddings, TensorflowHubEmbeddings, SagemakerEndpointEmbeddings, HuggingFaceInstructEmbeddings, SelfHostedEmbeddings, SelfHostedHuggingFaceEmbeddings, SelfHostedHuggingFaceInstructEmbeddings, FakeEmbeddings, AlephAlphaAsymmetricSemanticEmbedding, AlephAlphaSymmetricSemanticEmbedding: All of these classes are subclasses of BaseLLM.",
            "answer_properties": [],
            "notes": {
                "level": "high"
            }
        },
        {
            "id": "langchain-2",
            "question": "What classes are derived from the `Chain` class?",
            "reference_answer": "There are multiple classes that are derived from the Chain class. Some of them are:\n\nAPIChain\nAnalyzeDocumentChain\nChatVectorDBChain\nCombineDocumentsChain\nConstitutionalChain\nConversationChain\nGraphQAChain\nHypotheticalDocumentEmbedder\nLLMChain\nLLMCheckerChain\nLLMRequestsChain\nLLMSummarizationCheckerChain\nMapReduceChain\nOpenAPIEndpointChain\nPALChain\nQAWithSourcesChain\nRetrievalQA\nRetrievalQAWithSourcesChain\nSequentialChain\nSQLDatabaseChain\nTransformChain\nVectorDBQA\nVectorDBQAWithSourcesChain\n\nThere might be more classes that are derived from the Chain class as it is possible to create custom classes that extend the Chain class.",
            "answer_properties": [],
            "notes": {
                "level": "high"
            }
        },
        {
            "id": "langchain-3",
            "question": "What classes and functions in the `./langchain/utilities/` forlder are not covered by unit tests?",
            "reference_answer": "All classes and functions in the ./langchain/utilities/ folder seem to have unit tests written for them.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-4",
            "question": "What are the some of the main challenges that the Langchain project has to deal with?",
            "reference_answer": "Based on the context provided, it seems to be related to natural language processing and running models or chains on datasets. Some general challenges that could be faced in such projects may include:\n\n1. Handling large and diverse datasets: Processing and managing large datasets can be computationally expensive and time-consuming. Ensuring efficient data processing and storage is crucial for the project's success.\n\n2. Model performance and accuracy: Developing models that can accurately understand, generate, or manipulate natural language is complex. Ensuring high performance and accuracy is an ongoing challenge in the field of natural language processing.\n\n3. Integration with various tools and services: The project might require integration with different tools and services, such as language models, APIs, or other external resources. Ensuring seamless integration can be challenging.\n\n4. Scalability: As the project grows, it may need to handle more data, more models, or more users. Ensuring that the project can scale smoothly and handle increased workloads is important for long-term success.\n\n5. Security and privacy: Handling sensitive data and ensuring the security and privacy of users and their information is an essential aspect that needs to be addressed.\n\n6. Maintaining and updating models: Language models and other components of the project may need to be updated or maintained over time to ensure continued effectiveness and relevance.\n\n7. Cost management: Running language models, especially large ones, can be expensive. Managing the costs associated with running such models, along with other infrastructure and resources, is a challenge that needs to be addressed.\n\nPlease note that these are general challenges that could be faced in a natural language processing project. The specific challenges faced by the Langchain project might vary based on its goals, scope, and implementation.",
            "answer_properties": [],
            "notes": {
                "level": "high"
            }
        },
        {
            "id": "langchain-5",
            "question": "Does `UnstructuredMarkdownLoader` accept a string representation of a markdown file?",
            "reference_answer": "No, UnstructuredMarkdownLoader does not accept a string representation of a markdown file. It takes a file path as input.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-6",
            "question": "What is the purpose of the langchain project?",
            "reference_answer": "The purpose of the Langchain project is to provide a framework for developing applications powered by language models. It aims to simplify the creation of applications using large language models (LLMs) and offers a set of tools and services to assist developers in building NLP applications. The Langchain framework enables developers to create sequences of calls, connect language models to other data sources, and interact with application-specific data. It also provides off-the-shelf chains for specific tasks and allows for customization and building new chains. Overall, the Langchain project aims to revolutionize the way people learn languages and enhance the capabilities of language models in various applications.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-7",
            "question": "Is OpenAI's `gpt-4-0613` model supported?",
            "reference_answer": "No, this model is not supported.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-8",
            "question": "How do I use an `LLMChain` with a chat model? Please provide example code.",
            "reference_answer": "The LLMChain can be used with chat models as well:\n```\nfrom langchain import LLMChain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\n\nchat = ChatOpenAI(temperature=0)\n\ntemplate = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\nsystem_message_prompt = SystemMessagePromptTemplate.from_template(template)\nhuman_template = \"{text}\"\nhuman_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\nchat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n\nchain = LLMChain(llm=chat, prompt=chat_prompt)\nchain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n```",
            "answer_properties": [],
            "notes": {
                "level": "high"
            }
        },
        {
            "id": "langchain-9",
            "question": "Tell me about the `QAGenerationChain`.",
            "reference_answer": "The QAGenerationChain is a chain in LangChain that generates question-answer pairs over a specific document. It is a useful tool for generating evaluation data for question-answering tasks when you don't have a large dataset of examples. The QAGenerationChain takes a document as input and generates question-answer pairs that can be used to evaluate question-answering systems. This allows you to evaluate the performance of your question-answering system without the need for a large amount of pre-existing data.",
            "answer_properties": [],
            "notes": {
                "level": "low"
            }
        },
        {
            "id": "langchain-10",
            "question": "How do I reuse an existing `Qdrant` collection? Provide an example.",
            "reference_answer": "If you want to reuse the existing collection, you can always create an instance of Qdrant on your own and pass the QdrantClient instance with the connection details.\n\n```python\nimport qdrant_client\n\nclient = qdrant_client.QdrantClient(path=\"/tmp/local_qdrant\", prefer_grpc=True)\nqdrant = Qdrant(client=client, collection_name=\"my_documents\", embeddings=embeddings)\n```",
            "answer_properties": [],
            "notes": {
                "level": "high"
            }
        },
        {
            "id": "langchain-11",
            "question": "What does the `max_marginal_relevance_search` function do?",
            "reference_answer": "The `max_marginal_relevance_search` function returns a list of documents selected using the maximal marginal relevance algorithm, which optimizes for similarity to the query and diversity among the selected documents.",
            "answer_properties": [],
            "notes": {
                "level": "low"
            }
        },
        {
            "id": "langchain-12",
            "question": "What is the purpose of the `Qdrant` class?",
            "reference_answer": "The purpose of the Qdrant class is to serve as a wrapper around the Qdrant vector database.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-13",
            "question": "What does the `save_context` method of ConversationEntityMemory do?",
            "reference_answer": "The `save_context` method saves the context from the conversation history to the entity store by generating summaries for each entity in the entity cache.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-14",
            "question": "What is the use of the `ConversationBufferMemory` class?",
            "reference_answer": "The purpose of the ConversationBufferMemory class is to store conversation memory in a string buffer.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-15",
            "question": "What is a prompt template?",
            "reference_answer": "A prompt template refers to a reproducible way to generate a prompt. It contains a text string (\"the template\") that can take in a set of parameters from the end user and generates a prompt. A prompt template can include instructions to the language model, a set of few-shot examples to help the language model generate a better response, and a question to the language model. Prompt templates can be created using the PromptTemplate class in LangChain.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-16",
            "question": "Is there a caching mechanism for LLMs?",
            "reference_answer": "Yes, there is a caching mechanism for LLMs (Language Models). Caching can be used to save money by reducing the number of API calls made to the LLM provider and to speed up applications by reducing the number of API calls. There are different caching options available such as In Memory Cache, SQLite Cache, Redis Cache, Semantic Cache, GPTCache, Momento Cache, and SQLAlchemy Cache. These caching options can be used to cache prompts and responses, and some of them even support caching based on semantic similarity. Additionally, caching can be turned off for specific LLMs if desired.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-17",
            "question": "Give me a list of the main components of Langchain.",
            "reference_answer": "The main components of LangChain are as follows:\n\nModels: LangChain supports various model types and model integrations.\nPrompts: This module includes prompt management, prompt optimization, and prompt serialization.\nMemory: LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\nIndexes: This module covers best practices for combining language models with your own text data.\nChains: LangChain provides a standard interface for chains, integrations with other tools, and end-to-end chains for common applications.\nAgents: LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end-to-end agents.\n\nThese components can be used in various ways, including for personal assistants, question answering, chatbots, querying tabular data, interacting with APIs, extraction, summarization, and evaluation. Additionally, LangChain provides reference documentation for all its features and an ecosystem for integrating with other companies/products.",
            "answer_properties": [],
            "notes": {
                "level": "high"
            }
        },
        {
            "id": "langchain-18",
            "question": "Difference b/t a react agent and a plan-and-execute agent?",
            "reference_answer": "A ReAct agent and a plan-and-execute agent are two different types of agents in the LangChain framework.\n\nA ReAct agent, which stands for \"Reaction Agent,\" uses the ReAct framework to determine which tool to use based on the tool's description. It can use multiple tools and is designed to be used in conversational settings. It uses memory to remember previous conversation interactions and decides the next step using the history of tools, tool inputs, and observations. ReAct agents are suitable for small tasks and are wrapped in agent executors.\n\nOn the other hand, a plan-and-execute agent plans the full sequence of steps to accomplish an objective and then executes those steps in order. The planning is usually done by a language model (LLM), and the execution is done by a separate agent equipped with tools. Plan-and-execute agents are better suited for complex or long-running tasks that require maintaining long-term objectives and focus.\n\nIn summary, a ReAct agent uses the ReAct framework to decide which tool to use based on the tool's description and is designed for conversational settings. A plan-and-execute agent plans the full sequence of steps and executes them in order, making it suitable for complex tasks.",
            "answer_properties": [],
            "notes": {
                "level": "high"
            }
        },
        {
            "id": "langchain-19",
            "question": "Is there a `SelfQueryRetriever` for `Milvus`?",
            "reference_answer": "No. Self querying for Milvus is not supported.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-20",
            "question": "How do I define a custom tool?",
            "reference_answer": "To define a custom tool, you can use either the Tool dataclass or subclass the BaseTool class.\n\nUsing the Tool dataclass:\nImport the necessary modules: from langchain import Tool\nDefine the tool using the Tool dataclass, providing the required components such as name, description, and function.\nOptionally, you can define an args_schema to provide more information or validation for expected parameters.\nSubclassing the BaseTool class:\nImport the necessary modules: from langchain import BaseTool\nCreate a custom class that subclasses BaseTool.\nImplement the necessary methods and instance variables in your custom class.\nAdditionally, there is a @tool decorator provided that can be used to quickly create a Tool from a simple function. The decorator uses the function name as the tool name by default and the function's docstring as the tool's description.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        },
        {
            "id": "langchain-21",
            "question": "Is there an agent type that supports OpenAI's function calling interface?",
            "reference_answer": "Yes, there is an agent type that supports OpenAI's function calling interface. It is called the OpenAI Functions Agent. This agent is designed to work with OpenAI models that have been fine-tuned to detect when a function should be called and respond with the inputs that should be passed to the function. The OpenAI Functions Agent is capable of responding to prompts from the user using a Large Language Model.",
            "answer_properties": [],
            "notes": {
                "level": "mid"
            }
        }
    ]
}