Metadata-Version: 2.1
Name: flash-attention-srf
Version: 0.0.6
Summary: Softmax-less Flash Attention w/ SRFs
Home-page: https://github.com/alexjlevenston/flash-attention-srf
Author: Alexander Levenston
Author-email: alexlevenston2021@gmail.com
License: UNKNOWN
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: scipy
Requires-Dist: torch

# flash-attention-srf
An implementation of sequence-parallel Flash Attention, sans softmax, with SRFs.\
Makes [ERPTRI](https://erptri.com) go brrrrr.

### Installation
```bash
pip install --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly flash-attention-srf
```
### Usage
```python
import torch
from ops.torch.attention import Attention

B, H, L, D = 2, 4, 2 ** 10, 128
fast_attention = Attention(D, causal = True, device='cuda')

q, k, v = [torch.randn(B, H, L, D, dtype=torch.float16).cuda().requires_grad_() for i in range(3)]
output = fast_attention(q, k, v)

```


