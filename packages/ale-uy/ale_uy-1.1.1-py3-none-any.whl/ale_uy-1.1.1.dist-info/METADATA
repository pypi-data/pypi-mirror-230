Metadata-Version: 2.1
Name: ale-uy
Version: 1.1.1
Summary: herramienta para realizar limpieza, modelado y visualizacion de datos de manera sencilla y eficiente.
Home-page: https://github.com/ale-uy/DataScience
Author: ale-uy
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy ==1.22.2
Requires-Dist: pandas ==1.4.2
Requires-Dist: scikit-learn ==1.0.2
Requires-Dist: plotly ==5.7.0
Requires-Dist: matplotlib ==3.5.1
Requires-Dist: lightgbm ==3.3.2
Requires-Dist: xgboost ==1.5.0
Requires-Dist: catboost ==0.27.0
Requires-Dist: scipy ==1.8.0

## Modulo eda_vx.py: ManipulaciÃ³n de Datos

Las clases `eda_vx.EDA` y `eda_vx.Graph` son una herramienta poderosa para realizar manipulaciones y visualizaciones de datos de manera sencilla y eficiente. Estas clases estÃ¡n diseÃ±adas para facilitar diversas tareas relacionadas con el procesamiento y limpieza de los datos.

### MÃ©todos Disponibles

#### Preprocesamiento de Datos (EDA)

1. `eliminar_unitarios(df)`: Elimina las variables que tienen un solo valor en un DataFrame.

2. `eliminar_nulos_si(df, p)`: Elimina las columnas con un porcentaje de valores nulos mayor o igual a `p` en un DataFrame.

3. `imputar_faltantes(df, metodo="mm")`: Imputa los valores faltantes en un DataFrame utilizando el mÃ©todo de la mediana para variables numÃ©ricas y el mÃ©todo de la moda para variables categÃ³ricas. TambiÃ©n es posible utilizar el mÃ©todo de KNN (K-Nearest Neighbors) para imputar los valores faltantes.

4. `estandarizar_variables(df, metodo="zscore")`: Estandariza las variables numÃ©ricas en un DataFrame utilizando el mÃ©todo "z-score" (estandarizaciÃ³n basada en la media y desviaciÃ³n estÃ¡ndar). Tambien estan disponibles otros metodos de estandarizacion 'minmax' y 'robust'

5. `balancear_datos(df, target)`: Realiza un muestreo aleatorio de los datos para balancear las clases en un problema de clasificaciÃ³n binaria. Esto ayuda a mitigar problemas de desequilibrio de clases en el conjunto de datos.

6. `mezclar_datos(df)`: Mezcla los datos en el DataFrame de forma aleatoria, lo que puede ser Ãºtil para dividir los datos en conjuntos de entrenamiento y prueba.

7. `estadisticos_numerico(df)`: Genera datos estadÃ­sticos de las variables numÃ©ricas en el DataFrame.

8. `convertir_a_numericas(df, target, metodo="ohe")`: Realiza la codificaciÃ³n de variables categÃ³ricas utilizando diferentes mÃ©todos. Ademas de "ohe" (one-hot-encode) se puede seleccionar "dummy" y "label" (label-encode)

9. `all_eda(...)`: Pipeline para realizar varios pasos (o todos) de la clase de forma automatica.

#### VisualizaciÃ³n de Datos (Graph)

10. `graficos_categoricos(df)`: Crea grÃ¡ficos de barras horizontales para cada variable categÃ³rica en el DataFrame.

11. `grafico_histograma(df, x)`: Genera un histograma interactivo para una columna especÃ­fica del DataFrame.

12. `grafico_caja(df, x, y)`: Genera un grÃ¡fico de caja interactivo para una variable y en funciÃ³n de otra variable x.

13. `grafico_dispersion(df, x, y)`: Genera un grÃ¡fico de dispersiÃ³n interactivo para dos variables x e y.

14. `grafico_dendrograma(df)`: Genera un dendrograma que es Ãºtil para determinar el valor de k (grupos) para usar con la imputacion knn.

## Modulo ml_vx.py: Modelado de Datos

La clase `ml_vx.ML` es una herramienta poderosa para realizar modelados y visualizacion de datos de manera sencilla y eficiente. Esta clase estÃ¡n diseÃ±adas para facilitar diversas tareas relacionadas con el procesamiento, entrenamiento y evaluaciÃ³n de modelos de aprendizaje automÃ¡tico.

### Modelado de Datos
1. `modelo_lightgbm(...)`: Utiliza LightGBM para predecir la variable objetivo en un DataFrame. Este mÃ©todo admite problemas de clasificaciÃ³n y regresiÃ³n.

2. `modelo_xgboost(...)`: Utiliza XGBoost para predecir la variable objetivo en un DataFrame. Este mÃ©todo tambiÃ©n es adecuado para problemas de clasificaciÃ³n y regresiÃ³n.

3. `modelo_catboost(...)`: Utiliza CatBoost para predecir la variable objetivo en un DataFrame. Al igual que los mÃ©todos anteriores, puede manejar problemas de clasificaciÃ³n y regresiÃ³n.

> *IMPORTANTE*: si se pasa como parametro ``grid=True`` a cualquiera de estos modelos (ejemplo: **model_catboost(..., grid=True...)**), ahora se realiza una busqueda de hiperparametros **aleatoria** para reducir los tiempos de entrenamiento; ademas podemos pasar ``n_iter=...`` con el numero que deseemos que el modelo pruebe de convinaciones diferentes de parametros (10 es la opcion por defecto).

#### EvaluaciÃ³n de Modelos

5. **Metricas de ClasificaciÃ³n**: Calcula varias mÃ©tricas de evaluaciÃ³n para un problema de clasificaciÃ³n, como *precisiÃ³n*, *recall*, *F1-score* y Ã¡rea bajo la curva ROC (*AUC-ROC*).

6. **Metricas de RegresiÃ³n**: Calcula diversas mÃ©tricas de evaluaciÃ³n para un problema de regresiÃ³n, incluyendo el error cuadrÃ¡tico medio (MSE), el coeficiente de determinaciÃ³n (R-cuadrado ajustado), entre otros.

#### SelecciÃ³n de Variables

7. `importancia_variables(...)`: Calcula la importancia de las variables en funciÃ³n de su contribuciÃ³n a la predicciÃ³n, utiliza Bosque Aleatorio (RandomForest) con validacion cruzada. Utiliza un umbral que determina la importancia mÃ­nima requerida para mantener una variable o eliminarla.

8. `generar_clusters(df)`: Aplica el algoritmo no-supervisado K-Means o DBSCAN a un DataFrame y devuelve una serie con el nÃºmero de cluster al que pertenece cada observaciÃ³n.

9. `generar_soft_clusters(df)`: Aplica Gaussian Mixture Models (GMM) al dataframe para generar una tabla con las probabilidades de pertencia de cada observacion al cluster especifico.

10. `Graphs.plot_cluster(df)`: GrÃ¡fico de codo y silueta que es escencial para determinar el nÃºmero de clusters Ã³ptimo a utilizar en los mÃ©todos de clusters anteriores.

### InstalaciÃ³n

Para utilizar las clases `ML`, `EDA`, `Graph`, `Graphs`, `Tools`, simplemente importa la clase en tu cÃ³digo (copia la carpeta vx en el directorio en que estes trabajando):

```python
from vx.ml_vx import ML, Tools, Graphs
from vx.eda_vx import EDA, Graph
```
Recuerda asegurarte de tener las librerÃ­as necesarias instaladas en tu entorno (En anaconda solo necesitaras instalar *Lightgbm*, *Xgboost* y *Catboost*). Puedes instalar las librerÃ­as requeridas utilizando el siguiente comando en command-line (cmd, windows):

```bash
pip install -r requirements.txt
```

## Ejemplo de Uso
AquÃ­ tienes un ejemplo de cÃ³mo usar la clase **EDA** y **ML** para realizar un preprocesamiento de datos y entrenar un modelo de LightGBM para un problema de clasificaciÃ³n binaria (IMPORTANTE: Colocar los archivos **eda_vx.py** y **ml_vx.py** en la carpeta donde estes trabajando):

```python
# Importar la clase ML
from vx.ml_vx import ML, Tools, Graphs
from vx.eda_vx import EDA, Graph

# Cargar los datos en un DataFrame
data = pd.read_csv(...)  # Tu DataFrame con los datos

# Preprocesamiento de datos
preprocessed_data = EDA.all_eda(data, target='target')

# Entrenar el modelo LightGBM y obtener sus metricas
ML.modelo_lightgbm(preprocessed_data, target='target', tipo_problema='clasificacion')

# Si el modelo se adapta a nuestras necesidades, podemos guardarlo simplemente agregando el atributo save_model=True
ML.modelo_lightgbm(preprocessed_data, target='target', tipo_problema='clasificacion', save_model=True)
# Se guardara como "lightgbm.pkl"
```
Para usar el modelo guardado con nuevos datos, usaremos el siguiente codigo
```python
import joblib

# Ruta y nombre del archivo donde se guardÃ³ el modelo
model_filename = "nombre_del_archivo.pkl"
# Cargar el modelo
loaded_model = joblib.load(model_filename)
# Ahora puedes utilizar el modelo cargado para hacer predicciones
# Supongamos que tienes un conjunto de datos 'X_test' para hacer predicciones
y_pred = loaded_model.predict(X_test)
```
## ContribuciÃ³n
Si encuentras algÃºn problema o tienes ideas para mejorar estas clases, Â¡no dudes en contribuir! Puedes hacerlo enviando pull requests o abriendo issues en el repositorio del proyecto.

Â¡Gracias por tu interÃ©s! Espero que sea una herramienta Ãºtil para tus proyectos de aprendizaje automÃ¡tico. Si tienes alguna pregunta o necesitas ayuda, no dudes en preguntar. Â¡Buena suerte en tus proyectos de ciencia de datos y aprendizaje automÃ¡tico!
