Metadata-Version: 2.1
Name: ale-uy
Version: 1.1.3
Summary: herramienta para realizar limpieza, modelado y visualizacion de datos de manera sencilla y eficiente.
Home-page: https://github.com/ale-uy/DataScience
Author: ale-uy
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: catboost==1.2.1.1
Requires-Dist: contourpy==1.1.1
Requires-Dist: cycler==0.11.0
Requires-Dist: fonttools==4.42.1
Requires-Dist: graphviz==0.20.1
Requires-Dist: imbalanced-learn==0.11.0
Requires-Dist: imblearn==0.0
Requires-Dist: joblib==1.3.2
Requires-Dist: kiwisolver==1.4.5
Requires-Dist: lightgbm==4.1.0
Requires-Dist: matplotlib==3.8.0
Requires-Dist: numpy==1.26.0
Requires-Dist: packaging==23.1
Requires-Dist: pandas==2.1.0
Requires-Dist: Pillow==10.0.1
Requires-Dist: plotly==5.17.0
Requires-Dist: pyparsing==3.1.1
Requires-Dist: python-dateutil==2.8.2
Requires-Dist: pytz==2023.3.post1
Requires-Dist: scikit-learn==1.3.0
Requires-Dist: scipy==1.11.2
Requires-Dist: seaborn==0.12.2
Requires-Dist: six==1.16.0
Requires-Dist: tenacity==8.2.3
Requires-Dist: threadpoolctl==3.2.0
Requires-Dist: tzdata==2023.3
Requires-Dist: xgboost==2.0.0

### MÃ©todos Disponibles

#### Preprocesamiento de Datos (EDA)

1. `eliminar_unitarios(df)`: Elimina las variables que tienen un solo valor en un DataFrame.

2. `eliminar_nulos_si(df, p)`: Elimina las columnas con un porcentaje de valores nulos mayor o igual a `p` en un DataFrame.

3. `imputar_faltantes(df, metodo="mm")`: Imputa los valores faltantes en un DataFrame utilizando el mÃ©todo de la mediana para variables numÃ©ricas y el mÃ©todo de la moda para variables categÃ³ricas. TambiÃ©n es posible utilizar el mÃ©todo de KNN (K-Nearest Neighbors) para imputar los valores faltantes.

4. `estandarizar_variables(df, metodo="zscore")`: Estandariza las variables numÃ©ricas en un DataFrame utilizando el mÃ©todo "z-score" (estandarizaciÃ³n basada en la media y desviaciÃ³n estÃ¡ndar). Tambien estan disponibles otros metodos de estandarizacion 'minmax' y 'robust'

5. `balancear_datos(df, target)`: Realiza un muestreo aleatorio de los datos para balancear las clases en un problema de clasificaciÃ³n binaria. Esto ayuda a mitigar problemas de desequilibrio de clases en el conjunto de datos.

6. `mezclar_datos(df)`: Mezcla los datos en el DataFrame de forma aleatoria, lo que puede ser Ãºtil para dividir los datos en conjuntos de entrenamiento y prueba.

7. `estadisticos_numerico(df)`: Genera datos estadÃ­sticos de las variables numÃ©ricas en el DataFrame.

8. `convertir_a_numericas(df, target, metodo="ohe")`: Realiza la codificaciÃ³n de variables categÃ³ricas utilizando diferentes mÃ©todos. Ademas de "ohe" (one-hot-encode) se puede seleccionar "dummy" y "label" (label-encode)

9. `all_eda(...)`: Pipeline para realizar varios pasos (o todos) de la clase de forma automatica.

#### VisualizaciÃ³n de Datos (Graph)

10. `graficos_categoricos(df)`: Crea grÃ¡ficos de barras horizontales para cada variable categÃ³rica en el DataFrame.

11. `grafico_histograma(df, x)`: Genera un histograma interactivo para una columna especÃ­fica del DataFrame.

12. `grafico_caja(df, x, y)`: Genera un grÃ¡fico de caja interactivo para una variable y en funciÃ³n de otra variable x.

13. `grafico_dispersion(df, x, y)`: Genera un grÃ¡fico de dispersiÃ³n interactivo para dos variables x e y.

14. `grafico_dendrograma(df)`: Genera un dendrograma que es Ãºtil para determinar el valor de k (grupos) para usar con la imputacion knn.

### Modelado de Datos
1. `modelo_lightgbm(...)`: Utiliza LightGBM para predecir la variable objetivo en un DataFrame. Este mÃ©todo admite problemas de clasificaciÃ³n y regresiÃ³n.

2. `modelo_xgboost(...)`: Utiliza XGBoost para predecir la variable objetivo en un DataFrame. Este mÃ©todo tambiÃ©n es adecuado para problemas de clasificaciÃ³n y regresiÃ³n.

3. `modelo_catboost(...)`: Utiliza CatBoost para predecir la variable objetivo en un DataFrame. Al igual que los mÃ©todos anteriores, puede manejar problemas de clasificaciÃ³n y regresiÃ³n.

> *IMPORTANTE*: si se pasa como parametro ``grid=True`` a cualquiera de estos modelos (ejemplo: **model_catboost(..., grid=True...)**), ahora se realiza una busqueda de hiperparametros **aleatoria** para reducir los tiempos de entrenamiento; ademas podemos pasar ``n_iter=...`` con el numero que deseemos que el modelo pruebe de convinaciones diferentes de parametros (10 es la opcion por defecto).

#### EvaluaciÃ³n de Modelos

5. **Metricas de ClasificaciÃ³n**: Calcula varias mÃ©tricas de evaluaciÃ³n para un problema de clasificaciÃ³n, como *precisiÃ³n*, *recall*, *F1-score* y Ã¡rea bajo la curva ROC (*AUC-ROC*).

6. **Metricas de RegresiÃ³n**: Calcula diversas mÃ©tricas de evaluaciÃ³n para un problema de regresiÃ³n, incluyendo el error cuadrÃ¡tico medio (MSE), el coeficiente de determinaciÃ³n (R-cuadrado ajustado), entre otros.

#### SelecciÃ³n de Variables

7. `importancia_variables(...)`: Calcula la importancia de las variables en funciÃ³n de su contribuciÃ³n a la predicciÃ³n, utiliza Bosque Aleatorio (RandomForest) con validacion cruzada. Utiliza un umbral que determina la importancia mÃ­nima requerida para mantener una variable o eliminarla.

8. `generar_clusters(df)`: Aplica el algoritmo no-supervisado K-Means o DBSCAN a un DataFrame y devuelve una serie con el nÃºmero de cluster al que pertenece cada observaciÃ³n.

9. `generar_soft_clusters(df)`: Aplica Gaussian Mixture Models (GMM) al dataframe para generar una tabla con las probabilidades de pertencia de cada observacion al cluster especifico.

10. `Graphs.plot_cluster(df)`: GrÃ¡fico de codo y silueta que es escencial para determinar el nÃºmero de clusters Ã³ptimo a utilizar en los mÃ©todos de clusters anteriores.
