from docx import Document
import os
from typing import DefaultDict, List, Optional, Union, Dict, Any
import string
import pathlib
import json
from PyPDF2 import PdfFileReader
from IPython.display import IFrame
import PyPDF2
from IPython.display import Image
from IPython.display import display, Markdown
# import fitz

class Task:
    class Fields:
        ID = "id"
        UNIT = "unit"
        TASK_TEXT = "task_text"
        TASK_SOLUTION = "TASK_SOLUTION"

    def __init__(self, id: int = 0, unit: str = "test", task_text: str = "нет", task_solution: str = "нет"):
        self.id = id
        self.unit = unit
        self.task_text = task_text or "someshit"
        self.task_solution = task_solution

    @staticmethod
    def deserialize(data: Dict[str, Any]) -> 'Task':
        return Task(
            id=data.get(Task.Fields.ID),
            unit="a",
            task_text=data.get(Task.Fields.TASK_TEXT),
            task_solution=[data.get(Task.Fields.TASK_SOLUTION)]
        )
    


def help():
    print('1. econknow.find_by_words("любое количество слов")')
    print('2. econknow.get_task_by_id(выбранный_id)')
    print("""https://colab.research.google.com/drive/1-pgJ6hiDrEZxT0J3xTqw53SZmpt0tpD5?ouid=0&usp=drive_open#scrollTo=h4-0bfwhGYBX colab для практики, будьте аккуратны""")

def load_all_tasks():
    path = pathlib.Path(pathlib.Path(os.path.dirname(os.path.abspath(__file__)), "tasks.docx"))
    print(path)
    doc = Document(path)
    all_tasks = DefaultDict(list)
    last_task_id = None
    cnt = 0

    for para in doc.paragraphs:
        if para.style.name == 'Heading 1':
            last_task_id = para.text
            cnt += 1
        else:
            all_tasks[last_task_id] = [cnt]

    tasks = []

    cnt = len(tasks)
    for t in all_tasks:
        tasks.append(
            Task(
                id=cnt,
                task_text=t,
                task_solution=all_tasks[t]
            )
        )
        cnt += 1
    
    return tasks


def find_by_words(words: str):
    words = words.split()
    words = [w.lower().replace("ё","е") for w in words]
    all_tasks = load_all_tasks()
    counter = [0 for _ in range(max([task.id for task in all_tasks]) + 4)]
    for task in all_tasks:
        task_words = task.task_text.translate(str.maketrans('', '', string.punctuation))
        task_words = task_words.split(" ")
        task_words = [w.lower().replace("ё", "е") for w in task_words]
        for word in words:
            if word in task_words:
                counter[task.id] += 1
    all_tasks_by_id = {task.id: task for task in all_tasks}
    c = [[counter[i], i] for i in range(len(counter))]
    c.sort(reverse=True)
    for el in c:
        if el[0] > 0:
            i = el[1]
            task = all_tasks_by_id[i]
            text = task.task_text
            print(i, "\n".join([text[128*i:128*(i + 1)] for i in range(0, (len(text) - 1) // 128 + 1)]))



# _____________

def task_1():
    display(Markdown(r"""
**Эконометрика – быстроразвивающаяся отрасль науки, цель которой состоит в том, чтобы придать количественные меры экономическим соотношениям**.

Эконометрика является следсвием междисциплинарного подхода к изучению экономики и возникшая в результатет взаимодейсвий трех компонентов  

1) экономическая теория  

2) математические методы  

3) статистические методы  

**Предмет исследования эконометрики – экономические явления**.

**К основным задачам эконометрики можно отнести следующее**:

1) Построение эконометрических моделей, т.е. представление экономических моделей в математической форме, удобной для проведения эмпирического анализа. Данную проблему принято называть проблемной спецификации. Отметим, что зачастую она может быть решена несколькими способами.  

2) Оценка параметров построенной модели, делающих выбранную модель наиболее адекватной реальным данным. Это так называемый этап параметризации.

3) Проверка качества найденных параметров модели и самой модели в целом. Иногда этот этап анализа называют этапом верификации.

4) Использование построенных моделей для объяснения поведения исследуемых экономических показателей, прогнозирования и предсказания, а также для осмысленного проведения экономической политики.  


**Основные методы эконометрики**

1) Сводка и группировка информации;

2)  Анализ, который может быть вариационным и дисперсионным;

3) Применение регрессионного и корреляционного анализа;

4) Уравнения зависимостей

5) Индексы статистики.
        """))

def task_2():
    display(Markdown(r"""

В эконометрике существует 3 уровня измерения:

1. Низший (сравнение объектов по наличию или отсутствию исследуемых свойств) номинация, нумерация, классификация. Примером является шкала наименований
2. Средний (сравнение объектов по интенсивности проявляемых свойств) топология, упорядочение, шкалирование
3. Сравнение объектов с эталоном.
    """))


def task_3():
    display(Markdown(r"""
Этапы пред эконометрического моделирования:
1.	**Постановка задачи**:  
Определение конечных целей моделирования, набора участвующих в модели факторов и показателей, их роли;  


2.	**Предмодельный анализ (гипотеза и допущение)**    
Предмодельный анализ экономической сущности изучаемого явления, формирование и формализация априорной информации, в частности, относящейся к природе и генезису исходных статистических данных и случайных остаточных составляющих; 


3.	**Сбор информации**   
Сбор необходимой статистической информации, т.е. регистрация значений участвующих в модели факторов и показателей на различных временных или пространственных тактах функционирования изучаемого явления;



4.	**Спецификация модели**  
Формулирование вида модели, исходя из соответствующей теории связи между переменными. Определяется состав переменных и математическая функция для отражения связи между ними.  



5.	**Идентификация модели**  
Статистический анализ модели и в первую очередь статистическое оценивание неизвестных параметров модели;  

6.	**Верификация модели**  
Сопоставление реальных и модельных данных, проверка адекватности модели, оценка точности модельных данных.  

7.	**Интерпретация результатов**  


    """))


def task_4():
    display(Markdown(r"""
**4. Виды эконометрических моделей и типы переменных в модели.**


Виды эконометрических моделей:
1.	Регрессионные модели.
2.	Динамические модели [модели временных рядов]
3.	Системы эконометрических уравнений
Основные типы переменных в эконометрических моделях:
1.	Количественные (дискретные и непрерывные) и качественные.
2.	Эндогенные (зависимая, результативная, объясняемая) и экзогенные (независимая, факторная, объясняющее)
3.	Лаговые и предопределенные переменные

    """))


def task_5():
    display(Markdown(r"""
Парный корреляционный коэффициент показывает направление и степень тесноты связи между двумя признаками. Считается по формуле:


$$r_{xy} = \frac{\overline{xy} - \overline{x} \cdot \overline{y}}{\sqrt{Var(x) \cdot Var(y)}}$$

Оценка значимости осуществляется с помощью функции Фишера.

[r - коэффициент корреляции]
$$z = \frac{1}{2} \cdot ln(\frac{1 + r}{1 - r})$$
$t_{расч} = \frac{z}{S_z}$, где $S_z = \frac{1}{\sqrt{n - 3}}$

Если $|t_{расч}|$ > $t_{табл}(\alpha, \mu, n-\alpha)$, то существующая гипотеза отвергается и коэффициент корреляции является статистически значимым
 
Преобразования Фишера.

$$
Z' - t_{\gamma}\sqrt{\frac{1}{n-l-3}} <= Z <=Z' + t_{\gamma}\sqrt{\frac{1}{n-l-3}} $$

    """))


def task_6():
    display(Markdown(r"""
Индекс корреляции является нормированным показателем тесноты связи

$R = \sqrt{1 - \frac{Var_{ост}}{Var_{общ}}}$

$Var_{общ} = \sum\frac{(y_i - \overline{y})^2}{n-1}$

$Var_{ост} = \sum\frac{(y_i - \hat{y})^2}{n-2}$

Преобразования Фишера.

$$
Z' - t_{\gamma}\sqrt{\frac{1}{n-l-3}} <= Z <=Z' + t_{\gamma}\sqrt{\frac{1}{n-l-3}} $$
    """))


def task_7():
    display(Markdown(r"""
**7. Корреляционный анализ взаимосвязи качественных признаков: Индекс корреляции Оценка значимости. Интервальные оценки для значимых параметров связи**

$$r_s = 1-\frac{6}{n^3-n}\sum_{i=1}^{n}(x_i-y_i)^2$$

(русским языком: каждой цифровой или текстовой переменной присваиваем различные ранги, а затем по формуле вычисляем корреляцию)
Таким образом, расчет коэффициента корреляции Спирмена состоит из следующих шагов.
1. Сопоставить каждому признаку его порядковый номер (ранг) по возрастанию или убыванию значений.
2. Определить разности рангов для каждой пары сопоставляемых значений.
3. Возвести в квадрат каждую разность, затем просуммировать полученные результаты.
4. Вычислить коэффициент корреляции Спирмена по формуле 
Уравнение регрессии статистически значимо на уровне L= 0,05 , если Fp>=Fа;k1;k2 
Для значимых параметров связи имеет смысл найти интервальные оценки. При определении с надежностью у доверительного интервала для значимого парного или частного коэффициентов корреляции р используют Z-преобразование Фишера и предварительно


Индекс корреляции является нормированным показателем тесноты связи

$R = \sqrt{1 - \frac{Var_{ост}}{Var_{общ}}}$

$Var_{общ} = \sum\frac{(y_i - \overline{y})^2}{n-1}$

$Var_{ост} = \sum\frac{(y_i - \hat{y})^2}{n-2}$

Преобразования Фишера.

$$
Z' - t_{\gamma}\sqrt{\frac{1}{n-l-3}} <= Z <=Z' + t_{\gamma}\sqrt{\frac{1}{n-l-3}} $$
    """))


def task_8():
    display(Markdown(r"""
Каноническая корреляция - это распространение парной корреляции на случай, когда имеется несколько результативных показателей Y и несколько факторов X.   


Метод канонических корреляций является обобщением парной корреляции и позволяет находить максимальные корреляционные связи между двумя группами случайных величин. Эта зависимость определяется при помощи новых аргументов - канонических переменных, вычисленных как линейные комбинации исходных признаков по каждой из групп. Эти канонические величины должны максимально коррелировать между собой.  


Теснота связи между каноническими переменными будет определяться каноническими коэффициентами:

$$ r = \frac{cov(U, V)}{\sqrt{Var(U)Var(V)}}$$  

Значимость канонических корреляций проверяется с использованием критерия Пирсона $\chi^2$. Если вычислено m канонических корреляций $r_1, r_2,..., r_m$, то необходимо проверить m нулевых гипотез: $ H0_j: r_j=0, j=1, ...,m.$
При этом, необходимо учитывать, канонические корреляции упорядочены по величине $r_m  \geq r_{m-1} \geq... \geq r_1$.
При проверке гипотез статистика $\chi^2$ вычисляется по формуле:   

$$ \chi^2_j = (n - j - 0,5 * (k+1) + \sum{r_f^2}) * ln{(\prod{1-r_l^2})}$$

где n - объем выборки; 
k - размерность вектора $X=(X1,X2)^т$;   
$r_f$ – оценка f-го коэффициента канонической корреляции (f=1,...,j-1);   
$r_l$ – оценка l-го коэффициента канонической корреляции (l=1,...,m);   
число степеней свободы статистики: $ν=[(p-j+1)*(m-j+1)]$   


1) Если значение статистики $ \chi^2$ > критич. значения при заданном уровне значимости $\alpha$, то H0 отвергается и $r_j $ отлично от нуля, значит, корреляционная зависимость существует.  
2) Если p-Value <= $\alpha$, то H0 отвергается и $r_j $  отлично от нуля, значит, корреляционная зависимость существует.  
3) Так как значения канонических корреляций упорядочены, при $r_j = 0 $, то и остальные m – j значений канонических корреляций равны нулю.  

    """))


def task_9():
    display(Markdown(r"""
Парная регрессия (простая) модель, где среднее значение эндогенной переменно y рассматривается, как функция одной экзогенной еременной x: $M(y|x) = f(x) $ 

где  y – зависимая (объясняемая, эндогенная) переменная;
 x – независимая (объясняющая, экзогенная) переменная;
e – случайная (стохастическая) переменная, включающая влияние неучтенных в модели факторов.


1)Теоритическое уравнение парной линейной регрессии: $ y_i = \beta_0 + \beta_1 * x_i + \epsilon_i $, где  

$ \beta_0$ и $\beta_1$ - коэффициенты регресии   
$\epsilon_i$ - случайное отклонение (ошибка)   

2) Эмпирическое уравнение $ y_i = b_0 + b_1 * x_i + e_i $, где    

$ b_0$ и $b_1$ - оценки параметров
$ e_i = y_i - \hat{y_i}$ - ошибка   
$y_i$ - фактическое значение
$\hat{y_i}$ - теоритическое значение

Найденному коэффициенту  при объясняющем факторе  в парной линейной регрессии можно дать естественную экономическую интерпретацию. Коэффициент при объясняющем факторе показывает, на какую величину изменяется в среднем изучаемый эконометрический показатель при увеличении значения объясняющего фактора на одну единицу.
    """))


def task_10():
    display(Markdown(r"""
Данный класс нелинейных регрессий включает уравнения, в которых зависимая переменная линейно связана с параметрами. Примером могут служить:  
1) полиномы разных степеней (напрмер 3)

$ y_i = b_0 + b_1*x_i + b_2*x_i^2 + b_3*x_i^3 + e_i $

2) гипербола

$ y_i = b_0 + \frac{b_1}{x_i}+e_i $  

При оценке параметров регрессий нелинейных по объясняющим переменным используется подход, именуемый «замена переменных». Суть его состоит в замене «нелинейных» объясняющих переменных новыми «линейными» переменными и сведение нелинейной регрессии к линейной регрессии. К новой «преобразованной» регрессии может быть применен обычный метод наименьших квадратов (МНК).

Полином любого порядка сводится к линейной регрессии с ее методами оценивания параметров и проверки гипотез.

Среди нелинейной полиноминальной регрессии чаще всего используется парабола второй степени; в отдельных случаях — полином третьего порядка. Ограничение в использовании полиномов более высоких степеней связаны с требованием однородности исследуемой совокупности: чем выше порядок полинома, тем больше изгибов имеет кривая и, соответственно, менее однородна совокупность по результативному признаку.

Равносторонняя гипербола, для оценки параметров которой используется тот же подход «замены переменных» ($\frac{1}{x}$) заменяют на переменную z) 

Она может быть использована, например, для характеристики связи удельных расходов сырья, материалов и топлива с объемом выпускаемой продукции. Также примером использования равносторонней гиперболы являются кривые Филлипса и Энгеля.

Первая функция характеризует нелинейные соотношения между нормой безработицы x и процентом прироста заработной платы у. Из данной зависимости следует, что с ростом уровня безработицы темпы роста заработной платы в пределе стремится к нулю.

Вторая функция устанавливает закономерность – с ростом дохода доля расходов на продовольствие - уменьшается. Здесь у, обозначает - долю расходов на непродовольственные товары; х – доходы.


    """))


def task_11():
    display(Markdown(r"""
К данному классу регрессий относятся уравнения, в которых зависимая переменная нелинейно связана с параметрами. Примером таких нелинейных регрессий являются функции:  

1) степенная 

$ y_i = b_0 * x_i^{b_1} $  

2) показательная   

$ y_i = b_0 * b_i^{x_i} $ 

3) экспоненциальная  

$ y_i = e^{b_0+b_1*x_i}$  

Если нелинейная модель внутренне линейна, то она с помощью соответствующих преобразований может быть приведена к линейному виду (например, логарифмированием и заменой переменных). Если же нелинейная модель внутренне нелинейна, то она не может быть сведена к линейной функции и для оценки её параметров используются итеративные процедуры, успешность которых зависит от вида уравнений и особенностей применяемого итеративного подхода.  

Примером нелинейной по параметрам регрессии внутренне линейной является степенная функция, которая широко используется в эконометрических исследованиях при изучении спроса от цен: 
$ y_i = b_0 * b_i^{x_i} $  , где у — спрашиваемое количество; х — цена;  
Данная модель нелинейна относительно оцениваемых параметров, т. к. включает параметры а и b неаддитивно. Однако ее можно считать внутренне линейной, ибо логарифмирование данного уравнения по основанию е приводит его к линейному виду $ln(y_i) = ln(b_0) + b_1*ln(x_i)$ . Заменив переменные и параметры, получим линейную регрессию, оценки параметров которой $b_0$ и $b_1$ могут быть найдены МНК.
    """))


def task_12():
    display(Markdown(r"""
Метод наименьших квадратов(МНК)- метод, при котором рассчитывается сумма квадратов отклонений при котом рассчитывается сумма квадратов отклонений наблюдаемых значений результативной переменной у от теоретических значений $\hat{у}$ 

Классический подход к оцениванию параметров линейной регрессии основан на методе наименьших квадратов (МНК).

МНК позволяет получить такие оценки параметров a и b, при которых сумма квадратов отклонений фактических значений результативного признака y от расчетных $\hat{y}$ минимальна:  
$\sum{(y_i - \hat{y_i})^2} \longrightarrow min$  

Для того чтобы найти минимум функции, надо вычислить частные производные по каждому из параметров $b_0$ и $b_1$ и приравнять их к нулю. Тогда мы получаем следующую систему нормальных уравнений для оценки параметров $b_0$ и $b_1$  



$ \left\{ \begin{array}{rcl}
n*b_0+b_1*\sum{x} = \sum{y}
\\ 
b_0*\sum{x} + b_1*\sum{x^2} = \sum{x*y}
\\
\end{array}\right.
$

Решая систему нормальных уравнений либо методом последовательного исключения переменных, либо методом определителей, найдем искомые оценки параметров $b_0$ и $b_1$ 

    """))


def task_13():
    display(Markdown(r"""
**Функция правдоподобия**:  

Пусть плотность распределения генеральной совокупности $p(x, \theta)$ в точке x зависит от параметра $\theta$ и у нас имеется выборка $x_1, x_2, . . . , x_n$.
Рассмотрим совместную плотность выборки, которая равна произведению плотностей в силу независимости наблюдений:
$L(\theta) = p(x_1, θ) · . . . · p(x_n, \theta)$.  

Функция L называется функцией правдоподобия  

**Суть метода максимального правдоподобия**:  

Метод максимального правдоподобия состоит в том, чтобы при конкретных значениях выборки $x_1, x_2, . . . , x_n$ найти такое значение $\theta$, при котором функция L(θ) принимает максимальное значени

*Замечание*:

Оценка параметра зависит от выборки, так как при разных значениях $x_1, x_2, . . . , x_n$ могут получаться разные
значения оценки $\hat{\theta}$, то она является случайной величиной (а не просто числом).  

Как правило, при нахождении максимума функции правдоподобия  L рассматривают не её саму, а её логарифм $ln{L}$.
Связано это с тем, что логарифмируя функцию правдоподобия, произведение превращается в сумму и становится проще находить производную.
Максимумы L и $ln{L}$ достигаются при одном и том же значении параметра $\theta$.


**Теорема** 
Если для выборки объёма n выполнены условия регулярности, то   

1) решение  $\hat{\theta_n}$ единственно;  
2) $\hat{\theta_n}$ – состоятельная оценка параметра $\theta$;  
3) $\hat{\theta_n}$ – асимптотически нормальна с математическим ожиданием $\theta$   
4) ММП-оценка асимптотически эффективна.  





    """))


def task_14():
    display(Markdown(r"""
**Моменты**:

k-ым моментом случайной величины X является математическое ожидание этой случайной величины, возведенной в k-ую степень.
Таким образом, $E(X^k) = \mu_k$ – k-ый момент случайной величины X.  

**Суть метода моментов**: 

В силу того, что выборочные моменты являются состоятельными оценками популяционных и $\sqrt{n}*(\overline{X^k} - \mu^k)\rightarrow N(0: mu_{2k} - \mu^2_k ) $  представляется разумным искать оценки параметров из системы  

$\mu_k(\theta) = \overline{X^k}, k =1, ..., m$  

Процедура оценивания методом моментов состоит в приравнивании m популяционных моментов к m выборочным моментам для оценивания m неизвестных параметров модели $\theta_1, \theta_2, . . ., \theta_m$.  


*Свойства метода моментов*:  
1) Если все моменты существуют (конечны) и отображение, заданное системой, является биекцией (взаимно однозначно), то метод моментов обладает замечательными свойствами: состоятельность;  
2) асимптотическая нормальность (если $ \mu^{-1}$ гладкая).  


Обычно асимптотическая дисперсия оценки велика. В общем, оценки методом моментов в больших выборках сходятся к истинным значениям параметров, но их эффективность не гарантируется



    """))


def task_15():
    display(Markdown(r"""
Проверка значимости коэффициента $b_i$  означает проверку нулевой гипотезы: $H_0: b_i = 0$  
против альтернативной гипотезы: $H_1: b_i  \neq 0$   . 

Коэффициент  считают значимым (значимо отличающимся от нуля), если верна гипотеза $H_1$.

В качестве статистики для проверки гипотезы  принимается отношение коэффициента регрессии к его стандартной ошибке $t_{расч} = \frac{b_i}{S_i}$ . При выполнении исходных предпосылок модели эта величина имеет распределение Стьюдента с числом степеней свободы  $\nu = n-2$, где n – число наблюдений.  

Если модуль вычисленного по выборке (фактического) значения $t_{расч}$ статистики превысит критический уровень (табличное значение)$t_{табл}$,   

$\vert t_{табл} \vert  \geq t_{табл}$


то гипотезу $H_0$ следует отклонить на уровне значимости $\alpha$  и признать коэффициент $b_i$ значимым.

В противном случае, т. е. при $\vert t_{табл} \vert  \leq t_{табл}$ , коэффициент   $b_i$ незначим, и фактор x исключают из модели.
    """))


def task_16():
    display(Markdown(r"""
Проверить значимость уравнения регрессии — значит установить, соответствует ли математическая модель, выражающая зависимость между переменными, экспериментальным данным и достаточно ли включенных в уравнение объясняющих переменных (одной или нескольких) для описания зависимой переменной.

Проверка значимости уравнения регрессии производится на основе дисперсионного анализа. Дисперсионный анализ применяется как вспомогательное средство для изучения качества регрессионной модели.

Согласно основной идее дисперсионного анализа

$Var_{общ} = Var_{факт} + Var_{остат}$  

$TSS = RSS + ESS$  

$$ \frac{\sum^n_{i=1}{(y_i-\bar{y})^2}}{n-1} = \frac{\sum^n_{i=1}{(\hat{y}-\bar{y})^2}}{1} + \frac{\sum^n_{i=1}{(y_i-\hat{y})^2}}{n-2}$$   

Проверка значимости уравнения означает проверку нулевой гипотезы: $H_0: Var_{общ} = Var_{остат}  $  
против альтернативной гипотезы: $H_1: Var_{общ}  \neq  Var_{остат}  $   

$$ F_{расч} = \frac{Var_{факт}}{Var_{остат}}$$

$F_{табл} (\alpha, \mu_1=k, \mu_2=n=k-2)$

Если   $ F_{табл}   \geq F_{табл}$     то гипотезу $H_0$ следует отклонить на уровне значимости $\alpha$  и признать уравнение значимым.

    """))


def task_17():
    display(Markdown(r"""
Доверительный интервал прогноза парной регрессии — это построение нижней и верхней границ $y_{pmin}$ и  $y_{pmax}$ интервала, содержащего точную величину для прогнозного значения $\hat{y_{p}}$, что $y_{pmin} \lt \hat{y_{p}} \lt y_{pmax}$  

Доверительный интервал всегда определяется с заданной вероятностью (степенью уверенности),
соответствующей принятому значению уровня значимости $\alpha$.  

Предварительно вычисляется стандартная ошибка прогноза $m_{\hat{y}p}$  

$$m_{\hat{y}p} = \sigma_{} * \sqrt{1+\frac{1}{n}+ \frac{(x_p-\bar{x})^2}{\sum{(x-\bar{x})^2}}} $$  

Доверительный интервал

$$ \hat{y(x)} - t_p*\hat{\sigma}*\sqrt{1+\frac{1}{n}+ \frac{(x_p-\bar{x})^2}{\sum{(x-\bar{x})^2}}} \leq   y(x)  \leq \hat{y(x)} + t_p*\hat{\sigma}*\sqrt{1+\frac{1}{n}+ \frac{(x_p-\bar{x})^2}{\sum{(x-\bar{x})^2}}} $$

    """))


def task_18():
    display(Markdown(r"""
Множественная регрессия является обобщением парной регрессии. Она используется для описания зависимости между объясняемой (зависимой) переменой У и объясняющими (независимыми) переменными $Х_1,Х_2,…,Х_k$. Множественная регрессия может быть как линейная, так и нелинейная, но наибольшее распространение в экономике получила линейная множественная регрессия.  

$$ y_i = b_0 + b_1*x_1+ b_2*x_2 +...+ b_k*x_k + \epsilon $$


Как и в парной регрессии случайный член $\epsilon$ должен удовлетворять основным предположениям регрессионного анализа. Тогда с помощью МНК получают наилучшие несмещенные и эффективные оценки параметров теоретической регрессии. Кроме того переменные $Х_1,Х_2,…,Х_k$ должны быть некоррелированы (линейно независимы) друг с другом. 


Применение множественной регрессии позволяет исследователю ответить на вопрос, насколько хорошо оцененное уравнение аппроксимирует данные, есть ли значимая линейная связь, а также каковы оцененные значения коэффициентов для уравнения наилучшего предсказания. Кроме того, может быть определена относительная важность независимых переменных в предсказании зависимой переменной.

    """))


def task_19():
    display(Markdown(r"""
Несколько явлений могут быть соединены между собой нелинейными соотношениями. В этом случае для описания зависимостей следует воспользоваться множественной нелинейной регрессией.

1) полиномы разных степеней (напрмер 3)

$ y_i = b_0 + b_1*x_i + b_2*x_i^2 + b_3*x_i^3 + e_i $

2) гипербола

$ y_i = b_0 + \frac{b_1}{x_i}+e_i $  

Она может быть использована, например, для характеристики связи удельных расходов сырья, материалов и топлива с объемом выпускаемой продукции. Также примером использования равносторонней гиперболы являются кривые Филлипса и Энгеля.

**Пример:** кривая Филипса : $\pi = \pi^e - \gamma(u - u^*)+\epsilon$  
$\pi$ - темп инфляции   
$\pi^e $ - ожидаемый темп инфляции  
$ \gamma $ - коэффициент чувсвительности уровня инфлции к изменению уровня циклической безработицы  
$\epsilon$ - показатель шоковых изменений предложения  

    """))


def task_20():
    display(Markdown(r"""
Несколько явлений могут быть соединены между собой нелинейными соотношениями. В этом случае для описания зависимостей следует воспользоваться множественной нелинейной регрессией.

К данному классу регрессий относятся уравнения, в которых зависимая переменная нелинейно связана с параметрами. Примером таких нелинейных регрессий являются функции:  

1) степенная 

$ y_i = b_0 * x_i^{b_1} $  

2) показательная   

$ y_i = b_0 * b_i^{x_i} $ 

3) экспоненциальная  

$ y_i = e^{b_0+b_1*x_i}$    

**Пример**:
Кобба Дугласа: $Q= a * K^ß*L^∂$ 

Функция Кобба-Дугласа - модель с двумя переменными факторами производства. Параметр А — коэффициент, отражающий уровень технологической производительности, и в краткосрочном периоде он не изменяется. Показатели α и β - коэффициенты эластичности объема выпуска (Q) по фактору производства, т. е. по капиталу К и труду L соответственно.
    """))


def task_21():
    display(Markdown(r"""
Для изучения всязи между тремя и более переменными расчитывают следующие показатели 

1) Частный коэффициент корреяции характеризует направление и степень взаимосвязи и тесноты между двумя переменными при фиксированном значении остальных элементов  


$$ r_{y/x_1x2}= \frac{r_{yx_1}-r_{yx_2}*r_{x_2x_1}}{\sqrt{(1-(r_{yx_2})^2)*(1-(r_{x_1x_2})^2)}}$$    


2) Множественный коэффициент корреляции (коэффициент множественной корреляции, совокупный коэффициент множественной корреляции) ry характеризует степень тесноты линейной статистической связи (зависимости) результативного y и линейной комбинацией факторных x1, x2, . . ., xm признаков  


$$ r_{y/x_1x_2} = \frac{r_{yx_1}^2 + r_{yx_2}^2 - 2*r_{yx_1}*r_{yx_2}*r_{x_2x_1}}{1-r_{x_2x_1}^2}$$  

Индекс детерминации показывает долю вариации эндогенной переменной (y) обусловленной вариациями экзогенных перемнных

$$ R^2 = r_{y/x_1, ...,x_k}^2$$
    """))


def task_22():
    display(Markdown(r"""
Хорошая модель обладает такими свойствами:
1. Простота (скупость) – меньшее количество экзогенных переменных
2. Единственность – для одного набора данных статистических данных параметры идентифицируются однозначно
3. Максимальное соответствие – лучше то, где выше $ \overline{R^2} $ .
4. Согласованность с теорией
5. Хорошие прогнозные качества – средняя ошибка аппроксимации равна 5–7%
   
Улучшить модель можно путем предотвращения ошибок спецификации:
1. Отбрасывание значимой переменной
2. Добавление незначимой переменной
3. Неправильный выбор функциональной формы модели

Для обнаружения ошибок спецификации используется и тест Рамсея.

H0 – все коэффициенты в вспомогательной регрессии равны 0

Проверяем на основе F статистики: 

$$ \frac{\frac{RSS_{re} - RSS_{ur}}{2}}{\frac{RSS_{ur}}{n - N}} $$

, где $ RSS_{re} $ - RSS основной регрессии,
$ RSS_{ur}$  - вспомогательной, 
n - количество наблюдений,
N - количество переменных 

F крит(2, n - N)  должно быть больше F тест, что свидетельствует об отсутствии пропущенных переменных

    """))


def task_23():
    display(Markdown(r"""
Стандартизированное уравнение регресии это уравнение вида:
$y^0 = \beta_1x_1^0 + \beta_2x_2^0 + ... +\beta_px_p^0 +\varepsilon$

где  $y^0 = \frac{y-\overline{y}}{\sigma_y}$ и $x^0 = \frac{x_j-\overline{x_j}}{\sigma_{x_{j}}}$

В таком уравнение нивелирована размерность данных и является некоторым прообразом нормализации. Также это позволяет  утверждать о нормальности уравнения. 

Коэффициент регрессии - коэффециент при Х и свободный член, который приведен к стандартизированному виду (с нулевым математическим ожиданием ожиданием и единичным среднеквадратичным отклонением). Стандартизованные коэффициенты регрессии показывают, на сколько сигм изменится в среднем результат, если соответствующий фактор xj изменится на одну сигму при неизменном среднем уровне других факторов. 
    """))


def task_24():
    display(Markdown(r"""
Частные уравнения регрессии - это такое уравнение регрессии в котором один все кроме одного частного фактора фиксируются на среднем уровне/ 

$\widetilde{y}_{x2, x1} = a+b_1 \overline{x_1}+ b_2 x_2$

Это позволяет оенить влиянение частного фактора на отклик. Оценить влияние фактора позволяет и эластичность.

Коэффициент эластичности показывает, насколько во среднем изменится Y при изменении X на 1%. 
Э =  $f^\prime\left(x\right)\frac{x}{y}$

Коэффициент эластичности для гиперболы:
$\hat{y}=b_0+b_1\frac{1}{x}$

$y^\prime=-\frac{b_1}{x^2}\RightarrowЭ=-b1x2*xy$

    """))


def task_25():
    display(Markdown(r"""
Процесс получения оценок параметров эконометрической модели согласно методу прямого поиска реализуется на основе определенного алгоритма, логика которого состоит в следующем:

На первом этапе выбираются начальные оценки параметров эконометрической модели $a_i^{(0)}$, i=0,1,...,n; и их приращения $\Delta  a_i$. Для заданных значений параметров рассчитывается значение суммы квадратов ошибок $S_0^2=S^2(a^{(0)})$. Затем каждое исходное значение $a_i^{(0)}$ заменяется на следующий его вариант
$$a_i^{(1)}= a_i^{(0)}+ \Delta a_i$$

Для i=0,1,...,n; последовательно рассчитываются значения $S_0^2(a_i^{(1)})$. Если при этом $S_0^2(a_i^{(1)})<S_0^2$, то в качестве такого приближения выбирается значение $a_i^{(1)}=a_i^{(0)}–\Delta a_i$. Если оказывается, что и в том, и в другом случае сумма квадратов ошибки увеличивается, то в качестве оценки параметра $a_i$ эконометрической модели используется исходное значение $a_i^{(0)}$.

Таким образом, на первом шаге метода прямого поиска определяется направление движения к вектору “оптимальных” 
оценок a параметров модели по каждой координате. Значения приближений $a_i^{(2)}$ на втором шаге уже определяются 
по одной из двух формул $a_i^{(1)}=a_i^{(0)}±\Delta a_i$, и если $S_2^2(a_i^{(2)})< S_1^2$ , 1.
	То движение выбранном направлении продолжается в противном случае, т.е когда $S_2^2 \geq S_1^2$ в качестве оценки i-го параметра модели выбирается значение $a_i^{(1)}$. Процесс определения оптимальных оценок параметров экономической модели составляет определенное количество шагов до тех пор, пока изменение значений отдельных параметров в любом направлении не будут приводить к росту полученного на предыдущем этапе значения суммы квадратов ошибки $S_j^2$. j индекс характеризует номер предпоследнего этапа расчетов"""))


def task_26():
    display(Markdown(r"""
В основе этой группы методов лежит идея представления нелинейного функционала эконометрической модели f(a, x) в произвольной точке $a^(0)$ в виде линейной аппроксимирующей функции, например, ряда Тейлора. Это даст возможность определить в некотором смысле оптимальные приросты оценок параметров $\Delta a_i^{(1)}$, минимизирующие функцию суммы квадратов ошибки $S^2$ в окрестности точки $a^{(0)}$.


В практике оценивания параметров нелинейных эконометрических моделей наибольшее распространение получил метод, известный в научной литературе как метод Гаусса-Зайделя. Для него характерны следующие этапы расчетов.

1. Нелинейный функционал $f_t(a, x)$ в момент времени t и в точке оценок параметров $a^0$ в соответствии с аппроксимирующей функцией Тейлора представляется в следующем виде:
$$f_t(a,x) = f^0_t + (\frac{\partial f_t}{\partial \alpha_0})_0(a_0-a_0^0) + ... + (\frac{\partial f_t}{\partial \alpha_n})_0(a_n-a_n^0)$$
где $f^0_t = f_t(a^0, x)$- значение функционала в начальной точке оценок параметров $a^0 = (a_0^0,a_1^0, ..., a_n^0)$; $(\frac{\partial f_t}{\partial \alpha_i})_0$ расчетное значение производной функционала модели по параметру $a_i$ в точке $a_i^0$. На практике значения этих производных определяются на основе частных разностей. В итоге получаем
$$S^2 = \sum_{t} (y_t - f_t(\alpha, x))^2 = \sum_t (y_t-f_t^{(0)}-\sum_{i=0}^n(\frac{\partial f_t}{\partial  \alpha _i})_0*\Delta a_i^{(1)})^2$$
где $\Delta a_i^{(1)} = a_i^{(1)} - a_i^{(0)}; a_i^{(0)}$ - оценка параметра $\alpha_i$ на первом шаге расчетов.

Расчетные значения оценок $a_i^{(1)}, i=0,1,...,n$ можно получить, приравняв нулю первые производные функции $S^2$ по приростам $\Delta a_i$. В результате получим систему уравнений, аналогичных нормальным уравнениям классического метода наименьших квадратов:

$$\sum_{t=1}^T(y_t-f_t^{(0)}-(\frac{\partial f_t}{\partial \alpha_0})_0 * \Delta \alpha_0^{(1)}-(\frac{\partial f_t}{\partial \alpha_n})_0 * \Delta \alpha_n^{(1)}) * (\frac{\partial f_t}{\partial \alpha_0})_0 = 0 $$
.............................................................................
$$\sum_{t=1}^T(y_t-f_t^{(0)}-(\frac{\partial f_t}{\partial \alpha_0})_0 * \Delta \alpha_0^{(1)}-(\frac{\partial f_t}{\partial \alpha_n})_0 * \Delta \alpha_n^{(1)}) * (\frac{\partial f_t}{\partial \alpha_n})_0 = 0
$$
 
Из производных сформируем матрицу размерности Tx(n+1) следующего вида:

$$\widetilde{X}_0 = \begin{pmatrix}
\frac{\partial f_t}{\partial \alpha_0} & \frac{\partial f_t}{\partial \alpha_1} & ...  & \frac{\partial f_t}{\partial \alpha_n}\\ 
... & ... & ... & ... \\ 
\frac{\partial f_T}{\partial \alpha_0} & \frac{\partial f_T}{\partial \alpha_1} &  ... & \frac{\partial f_T}{\partial \alpha_n} 
\end{pmatrix}$$

Из приростов $\Delta a_i^{(1)}$ сформируем вектор-столбец 

Получаем преобразованное уравнение
$(\widetilde{X}_0'*\widetilde{X}_0) * \Delta \alpha^{(1)} = (\widetilde{X}_0' * g^0)$

Определив на основе полученных приростов $\Delta a_i^{(1)}, i=0,1,...,n$ оценки параметров эконометрической модели $a_i^1=a_i^0+\Delta a_i^1$, проделаем ту же процедуру расчетов для получения новых приближений этих оценок.
    """))


def task_27():
    display(Markdown(r"""
В основе методов оценки параметров эконометрической модели, предполагающих линеаризацию целевой функции, т. е. суммы квадратов ошибки модели $^S2(a, х)$ по переменным $a_0, a_1,...,a_n$, лежат свойства ее градиента $\triangledown S^2$, согласно которым направление этого вектора в произвольной многомерной точке пространства параметров $a_0^j, a_1^j,...,a_n^j$ указывает направление наибольшего роста функции $S^2(a, х)$ в этой точке. Соответственно противоположный вектор указывает на направление наибольшего уменьшения (наискорейшего спуска). Данный метод в объединении с методом Гаусса - Зейделя дает быстрое движение к оптимуму. 

Условимся, что $(\frac{\partial S^2 (a)}{\partial a_i})_{j-1} = -z_i^{(j-1)}$. Используя это 
$$ \Delta a^{(j)} = z^{j-1} / \lambda_j $$
где $\frac{1}{\lambda_j} = \frac{h^j}{\left | -\triangledown^2 * S^2 (a^{(j-1)}) \right |}$

Макуардт вывел: 

$(\widetilde{X}'_{(j-1)} * \widetilde{X}_{(j-1)} + \lambda^{(j)} * E)\Delta a^{(j)} = z^{j-1}$

где Е – единичная матрица, и предложил выбирать множитель $\lambda^{(j)}$, регулирующий длину прироста параметров j-м шаге расчетов, исходя из свойств функции $S^2(a^{(j –1)})$.

При хорошей обусловленности матрицы $(\widetilde{X}'_{(j-1)} * \widetilde{X}_{(j-1)})$, свидетельствующей о быстрой сходимости метода Гаусса-Зайделя, значение $\lambda^{(j)}$ выбирается относительно небольшим. Промежуточные значения $\lambda^{(j)}$ характеризуют направление движения к минимуму, полученное как комбинация направлений, определенных с помощью методов Гаусса-Зайделя и наискорейшего спуска.

В окрестности минимума метод Макуардта, как и другие методы, уменьшает длину прироста параметров.

Методы, основанные на линеаризации суммы квадратов ошибки эконометрической модели, также как и другие итеративные методы поиска оптимальных оценок ее параметров, могут привести к решению, соответствующему локальному оптимуму. Для определения глобального минимума необходимо, как и в случае других методов, решить задачу оценки с использованием разных вариантов исходных точек $a^0$, соответствующих разным участкам допустимой области существования искомых значений параметров.
    """))


def task_28():
    display(Markdown(r"""
Проверка остатков линейной регрессии на нормальность - позволяет проверить, соответствует ли применяемая модель регрессии исходным данным. В качестве тестов чаще всего используется тесты Хельвига и Шапиро-Уилка. Возможно и использование тестов Андерсона-Дарлинга, Шапиро-Франчиа, Хегази-Грина, Колмогорова-Смирнова.

Приведем тест Шапиро-Уилка:

Критерий Шапиро-Уилка основан на оптимальной линейной несмещённой оценке дисперсии к её обычной оценке методом максимального правдоподобия. Статистика критерия имеет вид:
$W=\frac{1}{s^2}\left[\sum_{i=1}^n a_{n-i+1} (x_{n-i+1} -x_i)\right]^2$,
где $s^2=\sum_{i=1}^n (x_i -\overline{x})^2, \overline{x}=\frac{1}{n}\sum_{i=1}^n x_i$.
    """))


def task_29():
    display(Markdown(r"""

    """))


def task_30():
    display(Markdown(r"""
В моделях, нелинейных по параметрам, например степенных или показательных, непосредственное применение МНК для их оценки невозможно, 
так как необходимым условием применимости МНК является линейность по коэффициентам уравнения регрессии. 
В данном случае преобразованием, которое приводит уравнение регрессии к линейному виду, является логарифмирование. 
Но на практике не всегда удается учесть влияние всех факторов на изучаемую переменную (например, в функции спроса учесть возрастные особенности потребителя), выбрать правильную форму математической зависимости между экономическими переменными (например, нелинейную вместо линейной), безошибочно выполнить измерения (правильно провести опрос). Поэтому необходимо включать некоторые случайные величины, называемые случайные возмущения. Выделяют два способа включения случ. возмущений в нелинейную модель, которая подразумевает и логарифмирование
1. Мультипликативное включение:

Исходная спецификация:

$Y_t = \alpha * X_t^{\beta} * \varepsilon_t$

Линеаризованная спецификация:

$Ln(Y_t) = Ln(\alpha) + Ln(X_t) *{\beta} + Ln(\varepsilon)$

Замена переменных:

$S_t=\beta_0+\beta_1*X_t+\nu_t$

Распределение вектора возмущений  $\nu_t = Ln(\varepsilon_t)$ - нормальное с параметрами:
$E(\nu_t)=0, Var(\nu_t)=\sigma^2$

$\varepsilon_t=\varepsilon^{\nu_t}$ логнормальное распределение с параметрами:
$E(\varepsilon_t)=\varepsilon^{\frac{\sigma^2}{2}}, Var(\varepsilon_t)=\varepsilon^{\sigma^2*(\sigma^2-1)}$


Исходная спецификация:

$Y_t = \alpha * X_t^{\beta} * \varepsilon^{\nu_t}$

Линеаризованная спецификация:

$Ln(Y_t) = Ln(\alpha) + Ln(X_t) *{\beta} + \nu_t$

Распределение вектора возмущений  $\nu_t$- нормальное с параметрами:

$E(\nu_t)=0, Var(\nu_t)=\sigma^2$

2. Аддитивное включение:

Исходная спецификация:

$Y_t = \alpha * X_t^{\beta} * \varepsilon_t$

Логарифмическое преобразование:

$Ln(Y_t) = Ln(\alpha* X_t^{\beta} + \varepsilon_t)$

Вывод:

Логарифмическое преобразование не приводит к линеаризации модели.
    """))


def task_31():
    display(Markdown(r"""
Основополагающая теорема, выдвинутая Карлом Гауссом и Андреем Марковым, о построении парной регрессии. Для парной регрессии можно получить МНК наилучшие линейные несмещённые оценки, если соблюдаются следующие условия:   

1. Математическое ожидание для всех испытаний 
$E\left(\varepsilon_i\right)=0,i=1,\overline{n}$

2. Дисперсия случайных отклонений постоянна для любых отклонений
$Var\left(\varepsilon_i\right)=Var\left(\varepsilon_j\right)=\sigma=const,\forall i,j$

Это свойство называется гомоскедастичностью

3. Случайные отклонения $\varepsilon_j$ и $\varepsilon_i$ являются независимыми для всех $i \neq j$

$\sigma_{\varepsilon_i,\varepsilon_j}=\left\{\begin{matrix}0&i\neq j\\\sigma^2,&i=j\\\end{matrix}\right.$

Отсутствие автокорреляции

Практическое примечание: сам коэф корреляции нормально распределен

4. Случайные отклонение независимы от объясняемой, экзогенной (объясняемой) переменной 
$\sigma_{\varepsilon\left(x_i\right)}=0$

5. Модель линейна относительно параметров

6. Отсутствие мультиколлинеарности

7. Случайные отклонения имеют нормальное распределение
$\varepsilon_i\sim N\left(0,\sigma\right)\ $

    """))


def task_32():
    display(Markdown(r"""
Параметры оценок, полученные МНК, обладают следующими свойствами BLUE оценки:

•	Несмещенность – нулевое математическое ожидание остатков

•	Эффективность – оценка характеризуется наименьшей дисперсией отклонений

•	Состоятельность – увеличение точности оценки при повышении размера выборки 

Состоятельная оценка – оценка, сходящаяся по вероятности к оцениваемому параметру

Несмещенная оценка – это оценка параметра, математическое ожидание которой равно значению оцениваемого параметра 

Эффективная оценка – это несмещенная оценка, имеющая наименьшую дисперсию из всех возможных несмещенных оценок данного параметра
    """))


def task_33():
    display(Markdown(r"""
Гетероскедастичность – непостоянство дисперсии отклонений. 

Причины:

•	Эффект масштаба

•	В пространственно-временных данных – эффект запаздывания данных

Последствия:

•	Оценки перестают быть эффективными

Увеличение дисперсии оценок снижает вероятность максимально точных оценок. Поэтому все выводы, полученные на основе t, F статистик и интервальные оценки будут ненадежными 

    """))


def task_34():
    display(Markdown(r"""
Графический:

Для квадратов отклонений (остатков) построить график зависимости случайных остатков $\varepsilon_i$ от факторов $x_i$. Если точки не имеют направленности, то это означает, что гетероскедастичности нет, то есть присутствует гомоскедастичность. Если расположение остатков на графике имеет определенную направленность, то это означает, что гетероскедастичность есть. Модель неадекватна.

Тест Бреуша-Пагана
H0: остатки гомоскедастичности $\gamma_2=\gamma_p=0$
1. Коэффициенты регрессии определяются МНК
2. Находим дисперсию ошибки модели $\widehat{\sigma}^2 = \frac{1}{n}RSS$
3. Вычисляем стандартизированные остатки 
$\frac{\varepsilon^2} {\widehat{\sigma}^2}$
4. Строится дополнительная регрессия квадратов стандартизированных ошибок на исходные наблюдаемые переменные:
$\widehat{\varepsilon}^2_t = \gamma_1 + \gamma_2z_{2t}+...+\gamma_pz_{pt}+ \eta_t$
5. LM=n*$R^2$,  где $R^{2}$ — коэффициент детерминации построенной на предыдущем шаге регрессии

При справедливости нулевой гипотезы о гомоскедастичности остатков статистика критерия имеет распределение хи-квадрат с p-1 степенями свободы.
    """))


def task_35():
    display(Markdown(r"""
При проверке по этому критерию предполагается, что стандартное отклонение $\sigma_i$ распределения вероятностей $e_i$ пропорционально значению $х$ в этом наблюдении. Предполагается также, что случайный член распределен нормально и не подвержен автокорреляции.Основная и альтернативная гипотезы в тесте Голфельда – Квандта (и во всех остальных тестах, в которых проверяется, имеет ли место гетероскедастичность) формулируются следующим образом: H0: гомоскедастичность H1: гетероскедастичность

Этапы теста Голдфелда-Квандта:

1. Все наблюдения $\textbf{n}$ упорядочиваются по величине $\textbf{x}$.

2. Вся упорядоченная выборка разбивается на 3 части: $\textbf{k}$, $\textbf{n-k}$, $\textbf{k}$
$$n=20 ->k=8$$
$$n=30 ->k=11$$
$$n=60 ->k=22$$

3. Оцениваются регрессии для первых $\textbf{k}$ наблюдений и для третьих $\textbf{k}$ наблюдений.

4. По каждой подвыборке рассчитывается сумма квадратов отклонений:
$$S_1=\sum_{i=1}^k e_i^2$$   $$S_3=\sum_{i=n-k+1}^n e_i^2$$

5. Рассчитывается $F_{расч}$-статистика Фишера: $F_{расч}=\frac{S_3}{S_1}$

6. Сравнение $F_{расч}$ и $F_{кр}(\alpha,v_1=V_2=k-m-1),=> гетероскедастичность$
    """))


def task_36():
    display(Markdown(r"""
При выполнении теста ранговой корреляции Спирмена предполагается, что дисперсия случай-
ного члена будет либо увеличиваться, либо уменьшаться по мере увеличения х, и поэтому в регрес-
сии, оцениваемой с помощью МНК, абсолютные величины остатков и значения х будут коррелиро-
ванны.

Алгоритм теста:
1. Данные по х упорядочиваются по возрастанию, каждому значению х присваивается ранг (rang x).
2. По данным столбца остатков е строится вспомогательный столбец |е| (модуль е) и также каж-
дому значению этого столбца присваивается ранг (rang |e|).
3. Считается коэффициент ранговой корреляции:

$r_{xe} = |1 - \frac{6\sum(D_{i}^2)}{n(n^2-1)}|,$ где $D_{i}$ – разность между рангом $х_{i}$ и рангом $e_{i}$ , т.е. $D_{i}$ = rang x - rang |e|, n – количество наблюдений в выборке.
4. Затем найденный коэффициент ранговой корреляции проверяется на значимость. Для этого
вычисляется статистический критерий $ t_{факт} = \frac{r_{xe}}{ \sqrt{1-r_{xe}^2}}  \sqrt{n-k-1}$ и сравнивается с $ t_{табл}$, которое берется из специальной таблицы «распределение Стьюдента» и находится на пересечении чисел $\gamma = 0.95$ или $\alpha = 0.95$ и $(n-k-1)$, где $k$ – число объясняющих переменных в задаче. Если $t_{факт} > t_{табл} $,то коэффициент корреляции значим, и в исследуемой модели присутствует ге-
тероскедастичность, и наоборот.
    """))


def task_37():
    display(Markdown(r"""
Тест ранговой корреляции Спирмена и тест Голдфельда–Кванта позволяют обнаружить лишь само наличие гетероскедастичности, но они не дают возможности проследить количественный характер зависимости дисперсий остатков регрессии от значений факторов и, следовательно, не представляют каких-либо способов устранения гетероскедастичности.

$\textbf{Тест Парка}$

1. Строится уравнение регрессии: $\widehat{y_i}=b_0+\sum_{j=1}^m b_jx_{ij}$

    и вычисляются остатки $e_i=y_i-\widehat{y_i}$, $i=1,..,n$

2. Выбирается фактор пропорциональности X  и оценивают вспомогательное уравнение регрессии: $ln(e_i^2)=\alpha_0 + \alpha_1*lnx_i + \eta_i$, i=1,..,n

3. Проверяют значимость коэффициента при $lnx_i$

$\textbf{Тест Глейзера}$

1. Строится уравнение регрессии: $\widehat{y_i}=b_0+\sum_{j=1}^m b_jx_{ij}$
    и вычисляются остатки $e_i=y_i-\widehat{y_i}$, $i=1,..,n$ 
    
2. Выбирается фактор пропорциональности X  и оценивают вспомогательное уравнение регрессии: $|e_i|=\alpha_0 + \alpha_1*x_i^\gamma + \eta_i$, i=1,..,n

3. Статистическая значимость коэффициента $\alpha_1$ в каждом случае означает наличие гетероскедастичности

4. Если для нескольких моделей будет получена значимая оценка $\alpha_1$, то характер гетероскедастичности определяют по наиболее значимой из них.

$\textbf{Тест Уайта}$

(на примере трех переменных) 
1. Cтроится уравнение регресси: $\widehat{y_i}=b_0+b_1x_{i1}+b_2x_{i2}+b_3x_{i3}$

    и вычисляются остатки: $e_i=y_i-\widehat{y_i}$, i=1,..,n

2. Оценивают вспомогательное уравнение регрессии: $e_i^2=\alpha_0+\alpha_1X_{i1}+\alpha_2X_{i2}+\alpha_3X_{i3}+\alpha_4X_{i1}^2+\alpha_5X_{i2}^2+\alpha_6X_{i3}^2+\alpha_7X_{i1}X_{i2}++\alpha_8X_{i1}X_{i3}++\alpha_9X_{i2}X_{i2}+\eta_i$

3. Определяют из вспомогательного уравнения тестовую статистику: $U=nR^2$

4. Проверяют общую значимость уравнения с помощью критерия $\chi^2$. Если $U > \chi_{\alpha;k}^2 $

   то гипотеза гомоскедастичности отвергается. Число степеней свободы k равно числу объясняющих переменных           вспомогательного уравнения. В частности, для рассматриваемого случая k=9
    """))


def task_38():
    display(Markdown(r"""

При подтверждении наличия гетероскедастичности необходимо преобразовать модели с целью смягчения ее влияния. Для этого предлагают вместо МНК применять МВНК(метод взвешенных наименьших квадратов).

Для устранения гетероскедастичности нужно найти способ придать наибольший вес наблюдению i, у которого среднее квадратическое отклонение случайной составляющей $\sigma(e_i)$ максимально (отметим, что такие наблюдения обладают самым низким качеством), и малый вес наблюдению, у которого среднее квадратическое отклонение случайной составляющей  $\sigma(e_i)$ минимально (такие наблюдения обладают самым высоким качеством). Тогда мы получим
более точные (эффективные) оценки параметров уравнения регрессии $\widehat{y_i}=b_0+b_1x_i+e_i$
    """))


def task_39():
    display(Markdown(r"""

МВНК(когда дисперсии случайных отклонений известны)

Запишем уравнение линейной регресии: $$y_i=\beta_0+\beta_ix_i+E_i$$

Разделим левую и правую часть на $\sigma_i$, где: $$\sigma_i=\sqrt{\sigma_i^2}$$

Получается уравнение вида: $$\frac{y_i}{\sigma_i}=\beta_0\frac{1}{\sigma_i}+\beta_1\frac{x_i}{\sigma_i}+\frac{E_i}{\sigma_i}$$

Введем новые переменные: $$y_i^{\ast}=\beta_0z_i+\beta_1x_i^{\ast}+v_i$$

Вектор случайных отклонений $v_i$ удовлетворяет условию гомоскедастичности.

Этапы МВНК:

1. Каждое из наблюдений $y_i, x_i$ делят на $\sigma_i$

2. По МНК для преобразовнных значений строятся уравнения регрессии с гарантированными качествами оценок регрессии
    """))


def task_40():
    display(Markdown(r"""

МВНК(когда дисперсия случайных отклонений неизвестна)

На практике $\sigma_i$ известна крайне редко, поэтому целесообразно сделать предположение что:

1. Дисперсии пропорциональны $x_i$, тогда $\sigma_i=\sqrt{x_i}$. Запишем уравнение:$$\frac{y_i}{\sqrt{x_i}}=\beta_0\frac{1}{\sqrt{x_i}}+\beta_1\frac{x_i}{\sqrt{x_i}}+\frac{E_i}{\sqrt{x_i}}$$

2. Дисперсии пропорциональны $x_i^2$, тогда $\sigma_i=x_i$
$$\frac{y_i}{x_i}=\beta_0\frac{1}{x_i}+\beta_1\frac{x_i}{x_i}+\frac{E_i}{x_i}$$
    """))


def task_41():
    display(Markdown(r"""

Под мультиколлинеарностью понимается тесная взаимная коррелированность объясняющих переменных $X_j$. Мультиколлинеарность может проявляться в функциональной (явной) и стохастической (скрытой) формах.
При функциональной форме мультиколлинеарности по крайней мере одна из парных связей между объясняющими переменными $X_j$  и $X_k$ является линейной функциональной зависимостью $r_{{X_j}X_k}$. В этом случае матрица $(X^TX)$ является особенной, так как содержит линейно зависимые векторы-столбцы и ее определитель равен нулю, что приводит к невозможности решения соответствующей системы нормальных уравнений и получения оценок параметров регрессионной модели.

В экономических исследованиях мультиколлинеарность чаще проявляется в стохастической форме, когда между хотя бы двумя объясняющими переменными $X_j и X_k$ существует тесная корреляционная связь. Матрица $(X^TX)$ в этом случае не является особенной, но ее определитель очень мал.

**Причины мультиколлинеарности:**

1. наличие причинно-следственных связей между социально-экономическими явялениями.

2. транзитивность 

**Последствия:**

Оценки полученные по МНК остаются несмещенными и эффективными, но перестают быть состоятельными

1. Большая дисперсия оценок, приводят к расширению интервала и снижению точности статистики полученных на основе их расчета

2. Затрудняется определение вклада каждой из объясняющих переменных в объясняемую переменную уравнение регрессии дисперсию зависимой переменной

3. Неверные знаки у коэффициентов регрессии

4. Незначимость коэффициентов регрессии в большем количестве при значимости уравнения в целом

5. Значительные изменения коэффициентов регрессии при изменении состава экзогенных переменных и объектов входящих в выборку
    """))


def task_42():
    display(Markdown(r"""

Еще один метод измерения мультиколлинеарности является следствием анализа формулы стандартной ошибки коэффициента регрессии:

$$s_{a_{j}} = \frac{\sigma_{y}}{\sigma_{x_{j}}}\sqrt{\frac{1-R^2_{yx1...x_{p}}}{(1-R^2_{x_{j}x_{1}...x_{j-1}x_{j+1}...x_{p}})(n-m-1)}}$$

Как следует из данной формулы, стандартная ошибка будет
тем больше, чем меньше будет величина, которую называют
фактор инфляции дисперсии (или фактор вздутия дисперсии) VIF:

$$VIF_{x_{j}} = \frac{1}{(1-R^2_{x_{j}x_{1}...x_{j-1}x_{j+1}...x_{p}})}$$
где $R^2_{x_{j}x_{1}...x_{j-1}x_{j+1}...x_{p}}$ — коэффициент детерминации, найденный для уравнения зависимости переменной х, от других переменных $х_{1}....х_{p}$,, входящих в рассматриваемую модель множественной регрессии.
Так как величина $R^2_{x_{j}x_{1}...x_{j-1}x_{j+1}...x_{p}}$
Отражает тесноту связи между переменной $х_{j}$ и прочими объясняющими переменными, то она, по сути, характеризует мультиколлинеарность применительно к данной переменной $х_{j}$. При отсутствии связи показатель $VIF_{x_{j}}$, будет равен (или близок) единице, усиление связи ведет к стремлению этого показателя к бесконечности. Считают, что если $VIF_{x_{j}} > 3$ для каждой переменной $Х_{j}$,то имеет место мультиколлинеарность.
    """))


def task_43():
    display(Markdown(r"""

Можно проводить проверку наличия мультиколлиниарности всего массива данных, используя статистику Фарррара-Глоубера, по формуле: $$FG_{набл}=-\big( n-1-\frac{1}{6}(2m+5)\big)ln(detR),$$
где n-число наблюдений;m-число объясняющих факторов; R-матрицы парных коэффициентов корреляции

Фактическое значение этого критерия сравнивается с табличным(критическим) значением с использованием статистики $\chi^2$. Значение $\chi^2(\alpha, \upsilon)$ зависит от уровня значимости $\alpha$ и числа степеней свободы $\upsilon=\frac{1}{2}m(m-1)$, здесь m-число объясняющих переменных.Критическое значение можно найти при помощи функции ХИ2ОБР MS EXcel
    """))


def task_44():
    display(Markdown(r"""

Применение ридж-регрессии(гребневой регрессии)предполагает корректировку элементов главной диагонали матрицы ($X^TX$) на некую произвольно задаваемую положительную величину $\tau$. Значение рекомендуется брать от 0,1 до 0,4. Дрейпер, Смит в своей работе приводят один из способов "автоматического" выбора величины $\tau$, предложенный Хоэрлом, Кеннардом и Белдвином: $$\tau=\frac{mSS_e}{n-m-1a^{{\ast}T}a^{\ast}}\space(1)$$

где m — количество параметров (без учета свободного члена) в исходной модели регрессии; $SS_e$ — остаточная сумма квадратов, полученная по исходной модели регрессии без корректировки на мультиколлинеарность; $a^{\ast}$ — вектор-столбец коэффициентов регрессии, преобразованных по формуле: $$a_j^{\ast}=a_j*\sqrt{\sum(x_j-\bar{x_j})^2}\space(2)$$

где $a_j$ — параметр при переменной $x_j$ в исходной модели регрессии.

После выбора величины $\tau$ формула для оценки параметров регрессии будет иметь вид: $$а_{\tau} = (Х_{\tau}^TX_{\tau} + {\tau}I)^{-1}Х_{\tau}^TY_{\tau}\space(3)$$

где I — единичная матрица; Х, — матрица значений независимых переменных: исходных или преобразованных по формуле 4; $Y_{\tau}$-вектор значений зависимой переменной: исходных или преобразованных по формуле (5).

При построении ридж-регрессии рекомендуется преобразовывать независимые переменные: $$ x_{{\tau}j}=\frac{x_j-\bar{x_j}}{\sqrt{\sum(x_j-\bar{x_j})}}\space(4)$$

и результативную переменную $$y_{\tau}=y-\bar(y)\space(5)$$

В этом случае после оценки параметров по формуле (3) необходимо перейти к регрессии по исходным переменным , используя соотношения $$a_j=\frac{a_{{\tau}j}}{\sqrt{\sum(x_j-\bar{x_j})^2}},\space j=1,2,..,p; \space a_0=\bar{y}-\sum_{j}a_j\bar{x_j}\space(6)$$

Оценки параметров регрессии, полученные с помощью формулы (3) будут смещенными. Однако так как определитель матрицы $(X^TX+{\tau}I)$ больше определителя матрицы $(X^TX)$, дисперсия оценок параметров регрессии уменьшится, что положительно повлияет на прогнозные свойства модели.
    """))


def task_45():
    display(Markdown(r"""


Абсолютно надёжным методом поиска наилучшего состава регрессоров из списка является
перебор всех возможны комбинаций. Существует несколько вариантов направленности
алгоритма
​
1. Включения (каждую итерацию включается по одному фактору)
2. Исключения (каждую итерацию исключается факторы)
3. Двунаправленный (сочетание 1 и 2 методов)


Целевой функцией для оптимизации является $R^2$ всей модели и $t$-статистика для каждого из
факторов
    """))


def task_46():
    display(Markdown(r"""

Применение метода главных компонент предполагает переход от взаимозависимых переменных **x** к независимым друг от друга переменным **z**, которые называют **главными компонентами**. Каждая главная компонента $z_j$ может быть представлена как линейная комбинация центрированных объясняющих переменных $t_j$. Центрирование переменной: $$t_{ji}^{\ast}=x_{ji}-\bar{x_j} \space(1)$$

Стандартизация(масштабирование): $$t_{ji}=\frac{x_{ji}-\bar{x_j}}{\sigma_{x_j}}  \space(2)$$

Количество компонент можеть быть меньше или равно количеству исходных независимых переменных **p**.Компоненту с номером **k** можно записать следующим образом: $$z_k=f_{k1}t_1+f_{k2}t_2+..+f_{kp}t_p      \space(3)$$

Доля дисперсии k-й компоненты в общей дисперсии независимых переменных рассчитывается по формуле: $$d_k=\frac{\lambda_k}{\sum_j\lambda_j}\space(4)$$

где $\lambda_k$ — собственное число, соответствующее данной компоненте; в знаменателе формулы (4) приведена сумма всех собственных чисел матрицы $\frac{1}{n}(T^TT)$, где T-матрица размером $(n*p)$, содержащая стандартизированные переменные.

После расчета значений компонент $z_j$ строят регрессию, используя МНК. Зависимую переменную в регрессии по главным
компонентам (5) целесообразно центрировать (стандартизовать) по формулам (1) или (2). $$t_y=b_1z_1+b_2z_2+..+b_kz_k+\delta \space(5)$$

где $t_y$ — стандартизованная (центрированная) зависимая переменная; $b_1,b_2,..,b_k$ — коэффициенты регрессии по главным компонентам; $z_1,z_2,..,z_к$ — главные компоненты, упорядоченные по убыванию собственных чисел  $\lambda_k$ ;$\delta$ -случайный остаток.

После оценки параметров регрессии (5) можно перейти к уравнению регрессии в исходных переменных, используя выражения (1)—(3).
    """))


def task_47():
    display(Markdown(r"""

Для устранения или уменьшения мультиколлинеарности используется ряд методов. Самый простой из них состоит в том, что из двух объясняющих переменных, имеющих высокий коэффициент корреляции ($r_{{x_j}x_k} \geq 0,8$), одну переменную исключают из рассмотрения.

При выборе следует руководствоваться правилом: исключаемая переменная должна при умеренной связи с результативным признаком быть тесно связана с одной и более объясняющими переменными.

Другой метод заключается в пошаговом исключении объясняющих переменных из уравнения на основе величины наблюдаемого значения t-критерия $t_{b_j}$ и стандартизованного коэффициента регрессии $\beta_j$. 

Кроме рассмотренной выше пошаговой процедуры исключения объясняющих переменных используется также пошаговая процедура включения переменных, основанная на поэтапном возрастании индекса детерминации $R^2$ с включением нового фактора в модель.

Следует отметить, что какая бы пошаговая процедура ни использовалась, она не гарантирует определения оптимального (с максимальным значением коэффициентом детерминации $R^2$) набора объясняющих переменных. Однако в большинстве случаев получаемые с помощью пошаговых процедур наборы переменных оказываются оптимальными или близкими к оптимальным
    """))


def task_48():
    display(Markdown(r"""
Автокорреляция - это наличие сильной корреляционной зависимости между последовательными наблюдениями одного ряда.

Виды автокорреляции:

$\cdot$ положительная (на графике проявляется чередованием зон положительных и отрицательных остатков).

$\cdot$ отрицательная (остатки "слишком часто" меняются).

Основными причинами автокорреляции являются:

$\cdot$ Спецификаци (неправильный выбор формы модели).

$\cdot$ Инерция.

$\cdot$ Эффект "Паутины" (реакция экономических показателей на изменение условий с запазданием).

Последствия автокорреляции:

$\cdot$ Aвтокорреляция не приводит к смещению оценок регрессии, но оценки перестают быть эффективными.

$\cdot$ Автокорреляция (особенно положительная) часто приводит к уменьшению стандартных ошибок коэффициентов, что влечет за собой увеличение t-статистик

$\cdot$ Оценка дисперсии остатков является смещенной оценкой истинного значения.

В силу вышесказанного выводы по оценке качества коэффициентов и модели в целом, возможно, будут неверными. Это приводит к ухудшению прогнозных качеств модели.
    """))


def task_49():
    display(Markdown(r"""
Автокорреляции остатков наблюдается тогда, когда значения предыдущих остатков завышают (положительная) или занижают (отрицательная) значения последующих.

Положительная автокорреляция на графике остатков проявляется в чередовании зон положительных и отрицательных остатков.

Отрицательная автокорреляция на графике выражается в том, что остатки «слишком часто» меняют знак
    """))


def task_50():
    display(Markdown(r"""
Последовательно определяются знаки отклонений et:

Ряд определяется как непрерывная последовательность одинаковых знаков. Количество знаков в ряду называется **длиной ряда**.

Визуальное распределение знаков свидетельствует о неслучайном характере связей между отклонениями. Если рядов слишком мало по сравнению с количеством наблюдений n, то вполне вероятна положительная автокорреляция. Если же рядов слишком много, то вероятна отрицательная автокорреляция.

При достаточно большом количестве наблюдений (n1 > 10, n2 > 10) и отсутствии автокорреляции СВ k имеет асимптотически нормальное распределение:

$$
M(k) = \frac{2n_+n_-}{n_++n_-}; D(k) = \frac{2n_+n_-(2n_+n_--n_+-n_-)}{(n_++n_-)^2(n_++n_--1)}
$$

$n_+$ - общее количество знаков “+” при n наблюдениях

$n_-$ - общее количество знаков “−” при n наблюдениях

k − количество рядов

Тогда, если $M(k) − u_{α/2} D(k) < k < M(k) + u_{α/2} D(k)$, то гипотеза об от-

сутствии автокорреляции не отклоняется.
    """))


def task_51():
    display(Markdown(r"""
Для определения автокорреляции в рассматриваемой модели можно использовать критерий Дарбина-Уотсона.

Ограничения Дарбина-Уотсона:

$\cdot$ Модель должна содержать свободный член.

$\cdot$ Данные должны иметь одинаковую периодичность.

$\cdot$ Дарбин-Уотсон не применим к моделям с автокорреляцией.

Статистика Дарбина-Уотсона:

$$
d = \frac{\sum^{n}_{t-2}\left(e_t-e_{t-1} \right)^2}{\sum^{n}_{t-2}e^2_t}
$$

Значение d-статистики сравнивается с критическими значениями d1 и d2. При этом могут возникнуть следующие ситуации:

$\cdot$ если $d_2<d<(4-d_2)$, то остатки признаются некоррелированными;

$\cdot$ если $0<d<d_1$, то имеется положительная автокорреляция;

$\cdot$ если $(4-d_1)<d<4$, то существует отрицательная автокорреляция;

$\cdot$ если $d_1<d<d_2$ или $(4-d_2)<d<(4-d_1)$, то это указывает на неопределенность ситуации.
    """))


def task_52():
    display(Markdown(r"""
Авторегрессионная схема первого порядка - метод устранения автокорреляции первого порядка между соседними членами ряда остатков в линейных моделях регрессии либо моделях регрессии, сводящихся к линейному виду.

Поскольку величина коэффициента автокорреляции неизвестна, то в качестве его оценки используется выборочный автокорреляционный коэффициент остатков первого порядка $p_1$:

$$
p_1 = \frac{\sum^T_{t=2}e_te_{t-1}}{\sum^T_{t=2}e^2_t}
$$

Имея (преположим) модель парной регрессии:

$$
y_t = \beta_0+\beta_1x_t+e_t
$$

Соответсвующая модель регрессии с учетом процесса автокорреляции остатков первого порядка:

$$
y_t=\beta_0+\beta_1x_t+pE_{t-1}+V_t
$$

$V_t$ - независимые, одинаково распределенные случайные величины с нулевым математическим ожиданием

Для получения регрессионной модели с учетом автокорреляции первого порядка необходимо вычесть из последнего уравнения тоже уравнение в момент времени $t-1$ домноженное на p. В итоге получится:

$$
y_t-py_{t-1} = \beta_0(1-p)+\beta_1(x_t-px_{t-1}) + V_t
$$
    """))


def task_53():
    display(Markdown(r"""
$$
Y* = \beta_1Y_{t-1}+\beta_2Y_{t-2}+E_t
$$


Модель авторегрессии второго порядка отличается от первой тем, что она включает в себя еще один влияющий фактор $y_{t-2}$, то есть показывается зависимость от того какой была объясняемая переменная не только один период назад, но и от того, какой она была два периода назад. Порой это позволяет выявить большую взаимосвязь и соответсвенно построить точный прогноз.
    """))


def task_54():
    display(Markdown(r"""
ρ - коэффициент авторегрессии

$$
DW = \frac{\sum^{n}_{t-2}\left(e_t-e_{t-1} \right)^2}{\sum^{n}_{t-2}e^2_t}
$$

$$
ρ \approx r = 1 - \frac{DW}{2}
$$
    """))


def task_55():
    display(Markdown(r"""
Процедура Кохрана-Оркатта:
    
Шаг 1: Оценка исходной модели методом наименьших квадратов и получение остатков модели
    
Шаг 2: Оценка коэффициента автокорреляции остатков модели
    
В качестве приближенного значения ρ берется его МНК-оценка: $e_t=ρe_{t-1}+V_t$

Шаг 3: Авторегрессионное преобразование данных (с помощью оцененного на втором шаге коэффициента автокорреляции) и оценка параметров преобразованной модели обычным МНК.


Процедура Хилдрета-Лу:

В данной процедуре производится прямой поиск значения коэффициента автокорреляции, которое минимизирует сумму квадратов остатков преобразованной модели. А именно задаются значения ρ из возможного интервала (-1;1) с некоторым шагом. Для каждого из них производится авторегрессионное преобразование, оценивается модель обычным МНК и находится сумма квадратов остатков. Выбирается тот коэффициент автокорреляции, для которого эта сумма квадратов минимальна. Далее в окрестности найденной точки строится сетка с более мелким шагом и процедура повторяется заново.
    """))


def task_56():
    display(Markdown(r"""
Метод можно использовать в том случае, когда есть основание считать, что автокорреляция отклонений очень велика. Для временных рядов обычно характерна положительная автокорреляция остатков р > 0. Поэтому при высокой автокорреляции полагают р = 1, и уравнение регрессии принимает вид:

$$
y_t-y_{t-1}=\beta_1(x_t-x_{t-1})+V_t
$$

$$
\Delta y_t=\beta_1\Delta x_t+V_t
$$

Коэффициент $\beta_1$ определяется обычным МНК.

Коэффициент доопределяется из соотношения:

$$
\beta_0=\overline{y}-\beta_1\overline{x}
$$
    """))


def task_57():
    display(Markdown(r"""
Фиктивная переменная — качественная переменная, принимающая значения 0 и 1, включаемая в эконометрическую модель для учёта влияния качественных признаков и событий на объясняемую переменную.

Пусть имеется признак, который принимает несколько возможных значений. Общее правило введения фиктивных переменных следующее: общее количество фиктивных переменных должно быть на единицу меньше количества возможных значений качественного признака, если в модели имеется константа. Это необходимо, чтобы не возникла проблема полной коллинеарности переменных.

Например, уровень образования: нет образования, среднее образование, высшее образование, ученая степень и т. д. В этом случае каждому уровню образования, кроме уровня «нет образования» можно поставить в соответствие некоторую фиктивную переменную.
    """))


def task_58():
    display(Markdown(r"""
Дисперсионный анализ — метод, направленный на поиск зависимостей в экспериментальных данных путём исследования значимости различий в средних значениях. В отличие от t-критерия, позволяет сравнивать средние значения трёх и более групп.

В зависимости от типа и количества переменных различают:

$\cdot$ однофакторный и многофакторный дисперсионный анализ (одна или несколько независимых переменных);

$\cdot$ одномерный и многомерный дисперсионный анализ (одна или несколько зависимых переменных);

$\cdot$ дисперсионный анализ с повторными измерениями (для зависимых выборок);

$\cdot$ дисперсионный анализ с постоянными факторами, случайными факторами, и смешанные модели с факторами обоих типов;

Ход дисперсионного анализа:

1) Подсчитать значение $SS_{\text{факт}}$ - означает вариативность признака, обусловленную действием исследуемого фактора

$$
SS_{\text{факт}} = \frac{\sum\sum x_{ij}}{n} - \frac{\left(\sum x_i\right)^2}{N}
$$

N - общее количество наблюдений

2) Подсчитать $SS_{\text{общ}}$ - общую вариативность признака.

$$
    SS_{\text{общ}} = \sum x_i^2 - \frac{\left(\sum x_i\right)^2}{N}
$$

3) Подсчитать случайную (остаточную) величину $SS_{\text{сл}} = SS_{\text{общ}}-SS_{\text{факт}}$

4) Опрелить число степеней свободы:

$$
df_{\text{факт}} = \text{кол-во объясняющих переменных} - 1
$$

$$
df_{\text{общ}} = \text{кол-во всех наблюдений} - 1
$$

$$
df_{\text{сл}} = df_{\text{общ}} - df_{\text{факт}}
$$

5) Разделить каждую SS на соответсвующую степень свободы

6) Посчитать значение $F_{\text{расч}} = \frac{MS_{\text{факт}}}{MS_{\text{сл}}}$ и сравнить его с табличным
    """))


def task_59():
    display(Markdown(r"""
Ковариационный анализ – методы изучения взаимосвязи между количественной зависимой переменной и набором категориальных и одновременно набором количественных предикторов. Независимые количественные предикторы в модели ковариационного анализа называют ковариатами, а категориальные независимые переменные – факторами.

Ковариационный анализ является как бы синтезом регрессионного и дисперсионного анализа. 
Основные теоретические и прикладные проблемы ковариационного анализа относятся к линейным моделям. 
Если в линейной модели взаимосвязи присутствуют только категориальные предикторы с помощью введения фиктивных переменных, 
то получается модель дисперсионного анализа. Если в линейной модели присутствуют только количественные предикторы – п
олучается модель регрессионного анализа. А при совместном введение факторов и ковариат проводится ковариационный анализ.

По отношению к зависимой переменной ковариаты являются сопутствующими переменными. Ковариационный анализ часто используется при «управлении»  эффектами внешних переменных. Другими словами, введение в модель ковариат позволяет оценить их влияние на взаимодействие зависимой переменной и факторов. Например, аналитик может использовать коэффициент IQ студентов в качестве ковариаты (количественный предиктор) при исследовании эффективности различных методов обучения (качественный предиктор).
    """))


def task_60():
    display(Markdown(r"""
Было бы здорово в процессе экономического моделирования учитывать влияние сезонной состовляющей => можно использовать фиктивные переменные

Фиктивные переменные - которые принимают значение 0 или 1

0 - отсутствие признака в наблюдении

1 - наличие признака в наблюдении

Моделирование сезонных переменных

Для этого воспользуемся следующими фиктивными переменными:

$\cdot d_1$ - 1 для первого квартала, 0 для других

$\cdot d_2$ - 1 для второго квартала, 0 для других

$\cdot d_3$ - 1 для третьего квартала, 0 для других

Состояние где фиктивные переменные нулевые - базовое

спецификация модели сдвига:

$$
y_i=\beta_1+\beta_2x_i+\beta_{d_1}d_1i+\beta_{d_2}d_2i+\beta_{d_3}d_3i+E_i
$$

$$
E(y_i|d_1=1) = (\beta_1+\beta_{d_1})+\beta_2x_i
$$

$$
E(y_i|d_2=1) = (\beta_1+\beta_{d_2})+\beta_2x_i
$$

$$
E(y_i|d_3=1) = (\beta_1+\beta_{d_3})+\beta_2x_i
$$

$$
E(y_i|d_1,d_2,d_3=0) = \beta_1+\beta_2x_i
$$

Экономический смысл коэффициентов заключается в том, насколько в среднем изменится $y_i$ при фиксированных значениях регрессора при переходе из сезона в сезон
    """))


def task_61():
    display(Markdown(r"""
Было бы здорово в процессе эконометрического моделирования изучать и учитывать не только количественные но и качественные переменные => можно использовать фиктивные переменные.

Фиктивные переменные - которые принимают значение 0 или 1

0 - отсутствие признака в наблюдении

1 - наличие признака в наблюдении

Спецификация модели с фиктивной переменной сдвига:

$$
y_t = \beta_1 + \beta_2x_t + \beta_dd_t + E_t
$$

$x_t$ - количественная переменная; $d_t$ - качественная характеристика

Экономическая характеристика:

$$
E(y_t|d_t=0)=\beta_1+\beta_2\cdot x_t
$$

$$
E(y_t|d_t=1)=(\beta_1+\beta_d)+\beta_2x_t
$$


$\beta_d$ (параметр при фиктивной переменной сдвига) - 
среднее изменение изучаемого признака $y$ при переходе из одной категории в другую при неизменяемых значениях остальных параметров. Влияет только на изменение свободного члена в уравнении регрессии, поэтому и называется сдвигом.
    """))


def task_62():
    display(Markdown(r"""
Фиктивная переменная наклона изменяет наклон линии регрессии.

Фиктивные переменные - которые принимают значение 0 или 1

0 - отсутствие признака в наблюдении

1 - наличие признака в наблюдении

Фиктивные переменные наклона дают возможность построить линейные модели переменного наклона (кусочно-линейные), позволяющие учесть структурные изменения в экономических процессах.

Спецификация модели с фиктивной переменной наклона:

$$
y_t = \beta_0 + \beta_1x + \beta_2\gamma x + E
$$

    """))


def task_63():
    display(Markdown(r"""
В экономике структурные изменения — это сдвиг или изменение основных способов функционирования рынка или экономики.

Структурные изменения в экономике могут быть вызваны следующими факторами:

$\cdot$ экономическое развитие страны

$\cdot$ глобальные сдвиги в капитале и рабочей силе

$\cdot$ изменения в доступности ресурсов вследствие войны или стихийных бедствий, открытия или истощения природных ресурсов

$\cdot$ изменений в политической системе

Фиктивная переменная — качественная переменная, принимающая значения 0 и 1, включаемая в эконометрическую модель для учёта влияния качественных признаков и событий на объясняемую переменную.

Тест Чоу - это процедура проверки стабильности параметров регрессионной модели, наличия структурных сдвигов в выборке. Фактически тест проверяет неоднородность выборки в контексте регрессионной модели.

![image.png](attachment:image.png)

Модель для первого набора наблюдений:

$$
Y = \beta^{'}_0+\beta^{'}_1X_1+...+\beta^{'}_kX_k+\epsilon^{'}   
$$

Модель для второго набора наблюдений:

$$
Y = \beta^{''}_0+\beta^{''}_1X_1+...+\beta^{''}_kX_k+\epsilon^{''}   
$$

$$
H_0: \beta^{'}_0 = \beta^{''}_0, ..., \beta^{'}_k = \beta^{''}_k, \sigma^2_{\epsilon^{'}} = \sigma^2_{\epsilon^{''}}
$$

$$
H_1: \exists i: \beta^{'}_i \neq \beta^{''}_i
$$

Тестовая статистика в тесте Чоу:

$$
F = \frac{(RSS_R-RSS_{UR})/(k+1)}{(RSS_1+RSS_2)/(n-2(k+1))} = \frac{\left(RSS_p - (RSS_1+RSS_2)\right)/\left(k+1\right)}{(RSS_1+RSS_2)/(n-2(k+1))}
$$

k - кол-во всех регрессоров

$RSS_p$ - это сумма квадратов остатков для всей выборки

$RSS_1$ - это сумма квадратов остатков для выборки 1

$RSS_2$ - это сумма квадратов остатков для выборки 2

Если F > Fкритическое (при выбранном уровне значимости), то основная гипотеза отвергается и нужно оценивать две отдельные регрессии.
    """))


def task_64():
    display(Markdown(r"""
Модель линейной вероятности

$
Y_i = \beta_0+\beta_1X_{1i}+...+\beta_kX_{ki}+\epsilon_i
$

$
Y_i=E(Y_i)+\epsilon_i
$

Найдем математическое ожидание $Y_i$:

$
p_i=p(Y_i=1)
$

$
E(Y_i) = 1\cdot p_i+0\cdot(1-p_i)=p_i=\beta_0+\beta_1X_{1i}+...+\beta_kX_{ki}
$

$$
p_i = p(Y_i=1)=\beta_0+\beta_1X_{1i}+...+\beta_kX_{ki}
$$

Если мы будем оценивать модель с качественной зависимой переменной, как и ранее, с помощью МНК, мы получим указанную выше модель, называемую моделью линейной вероятности.

Недостатки линейной верочтностной модели:

$\cdot$ оцененные значения вероятности могут оказаться больше 1 или меньше 0

$\cdot$ Распределение случайного члена не является нормальным

$\cdot$ имеет место проблема гетероскедастичности
    """))


def task_65():
    display(Markdown(r"""
$$
y_t = a+bX_t+\epsilon_t, t=1,...,n
$$

МНК-оценка для парной линейной регрессии

$
\hat b=\frac{\overline{x\cdot y}-\overline{x}\cdot\overline{y}}{\overline{x^2}-\overline{x}^2}=\frac{\hat{cov}(X,Y)}{\hat{Var}(x)}\overline{x}
$

$\hat{cov}(X,Y)=\frac{1}{n}\sum^n_{i=1}(x_i-\overline{x})(y_i-\overline{y})$ - выборочная ковариация

$\hat{Var}(x)=\frac{1}{n}\sum^n_{i=1}(x_i-\overline{x})^2$ - выборочная дисперсия

Свойства МНК-оценок:

(Базовые (верны даже при мал. выборках и без предположения о нормальности $\epsilon_i$)

- оценки линейны по y

- оценки несмещенные: $E(\hat{\beta}_j|X)=\beta_j$

- оценки эффективны среди линейных и несмещенных

- $Var(\hat{b})=\frac{\sigma^2}{\sum(x_i-\overline{x})}$

- $cov(\hat{a},\hat{b}|X)) = -\overline{X}\frac{\sigma^2}{\sum(x_i-\overline{x})^2}$

(Ассимтотичесике свойства (верны при $n\rightarrow\infty$ и без предположения о нормальности))

- оценки состоятельны

- $\frac{\hat{\beta_j}-\beta_j}{se(\hat{\beta_j})}\rightarrow N(0;1)$; $\hat{\beta_j}$ - стандартная ошибка

при нормальности ошибок оценки эффективны среди несмещенных
    """))


def task_66():
    display(Markdown(r"""
Множественная регрессиия - это модель где среднее значение эндогенной переменной (y) представлено как функция множества экзогенных переменных ($x_1,x_2,...$)

$$
y = \beta_0+\beta_1x_1+\beta_2x_2+...+\beta_mx_m+\epsilon
$$

F-тест качества спецификации множественной регрессионной модели (Проверка надежности множественной регрессии):

$$
H_0: Var_{общ} = Var_{ост}
$$

$$
H_1: Var_{общ} \neq Var_{ост}
$$

$
F_{расч} = \frac{R^2}{1-R^2}\cdot \frac{n-k-1}{n-1}
$

$
R - \text{коэффициент детерминации} = \left(r_{y/x_1x_2}\right)^2
$

$
n - \text{количество наблюдений; } k - \text{количество экзогенных переменных}
$

В случае если $H_0$ отвергается ($F_{расч} > F_{табл}$) уравнение регрессии является надежным.
    """))


def task_67():
    display(Markdown(r"""
Типы нелинейности эконометрических моделей:

Гиперболическая модель: $y=\beta_0+\frac{\beta_1}{x}+\epsilon$
        
Логарифмическая модель: $y=\beta_0+\beta_1ln(x)+\epsilon$

Степенная модель: $y=\beta_0\cdot x^{\beta_1}\epsilon$

Показательная: $y=\beta_0\cdot e^{\beta_1\cdot x}\epsilon$

Классы нелинейных эконометрических моделей:

$\cdot$ нелинейные, относительно объясняющих переменных, но линейные по оцениваемым параметрам (гиперболическая)

$\cdot$ нелинейные, относительно объясняющих переменных и нелинейные по оцениваемым параметрам (степенная, показательная)

| Вид зависимости | Ограничение | Замена переменных |   | Обратная замена |   |
|-----------------|-------------|-------------------|---|-----------------|---|
|Гиперболическая|$x\neq0$| $g=y$ | $t=\frac{1}{x}$|$a=c$|$b=d$|
|Логарифмическая|$x>0$| $g=y$ | $t=ln(x)$ |$a=c$|$b=d$|
|Степенная       |$x,y>0$| $g=ln(y)$ | $t=ln(x)$|$a=e^c$|$b=d$|
|Показательная|$y>0$| $g=ln(y)$ |$t=x$|$a=e^c$|$b=d$|

Для оценки параметров данных регрессионных моделей используют:

$\cdot$ МНК

$\cdot$ Метод моментов

$\cdot$ Метод максимального правдоподобия
    """))


def task_68():
    display(Markdown(r"""
Гетероскедастичность - непостоянство дисперсий отклонений.

Причины:

$\cdot$ Эффект масштаба пространсвенно временных рядов

$\cdot$ Эффект запаздывания данных

$\cdot$ Эффект запаздывания данных

Последствие:

$\cdot$ Оценки перестают быть эффективными

При подтверждении наличия гетероскедастичности необходимо преобразовать модель с целью смягчения ее влияния. Для этого предлагают применять МВНК (метод взвешенных наименьших квадратов).

Этапы МВНК:

$\cdot$ Каждое из наблюдений $y_i$, $x_i$ делят на известную величину $\sigma_i$ (дисперсия случайных отклонений известна - $\sigma_i=\sqrt{\sigma_j^2}$; дисперсия случайнх отклонений неизвестна - $\sigma_i = \sqrt{x_i}$ при пропорциональности дисперсии и $x_i$ или $\sigma_i = x_i$ при пропорциональности дисперсии и $x_i^2$)

$\cdot$ По МНК для преобразованиязначений строится уравнение регрессии с гарантированным качеством оценок (остатки гомоскедастичны)
    """))


def task_69():
    display(Markdown(r"""
Типы нелинейности эконометрических моделей:

Гиперболическая модель: $y=\beta_0+\frac{\beta_1}{x}+\epsilon$
        
Логарифмическая модель: $y=\beta_0+\beta_1ln(x)+\epsilon$

Степенная модель: $y=\beta_0\cdot x^{\beta_1}\epsilon$

Показательная: $y=\beta_0\cdot e^{\beta_1\cdot x}\epsilon$

Классы нелинейных эконометрических моделей:

$\cdot$ нелинейные, относительно объясняющих переменных, но линейные по оцениваемым параметрам (гиперболическая)

$\cdot$ нелинейные, относительно объясняющих переменных и нелинейные по оцениваемым параметрам (степенная, показательная)

| Вид зависимости | Ограничение | Замена переменных |   | Обратная замена |   |
|-----------------|-------------|-------------------|---|-----------------|---|
|Гиперболическая|$x\neq0$| $g=y$ | $t=\frac{1}{x}$|$a=c$|$b=d$|
|Логарифмическая|$x>0$| $g=y$ | $t=ln(x)$ |$a=c$|$b=d$|
|Степенная       |$x,y>0$| $g=ln(y)$ | $t=ln(x)$|$a=e^c$|$b=d$|
|Показательная|$y>0$| $g=ln(y)$ |$t=x$|$a=e^c$|$b=d$|

Для оценки параметров данных регрессионных моделей используют:

$\cdot$ МНК

$\cdot$ Метод моментов

$\cdot$ Метод максимального правдоподобия
    """))


def task_70():
    display(Markdown(r"""
Последствием ошибочного выбора типа функции в уравнении регрессии является нарушение предпосылки о нулевом ожидаемом значении случайного остатка. При оценивании модели функции регрессии любым методом по обучающей выборке оказывается нарушенной предпосылка теоремы Гаусса —Маркова.

В итоге оценки коэффициентов модели оказываются смещенными, а ее точность становится неактуальной. В конечном счете прогноз значения у, вычисленный по оцененной модели с ошибочной функцией регрессии оказывается неадекватным в силу того, что в основе прогноза прежде всего лежит описанное выше допущение.

Симтомы неверно выбранного типа функции регрессии:

$\cdot$ несоответствии диаграммы рассеяния, построенной по выборке, графику функции.

$\cdot$ длительное постоянство знака оценок случайных остатков в упорядоченных (по возрастанию значений объясняющей переменной) уравнениях наблюдений.

Решением данной проблемы является выбор иной - более подходящей функции и повтор регрессионного анализа.
    """))


def task_71():
    display(Markdown(r"""
Гомоскедастичность есть постоянство дисперсий всех наблюдений, а так же равенство матожидания этих остатков нулю (условия Геусса-Маркова)

Для проверки исходных данных да гомоскедастичность существует множество специальных тестов. Одним из них является тест Голдфелда-Квандта.

Тест Голдфелда-Квандта:

Шаг 1.

Все наблюдения упорядочиваются по величине x.

Шаг 2.

Вся выборка в начале и уонце делится на две части. Количество наблюдений в этих подвыборках одинаково и определяется в соответсвии изначальному количеству наблюдений.

Шаг 3.

Получившиеся выборки оцениваются регрессией.

Шаг 4.

По каждой подвыборке рассчитывается сумма квадратов отклонений/

Шаг 5.

Расчитывается $F_{расч}$-статистика Фишера:

$$
F_{расч} = \frac{S_{бол}}{S_{мен}} \text{, где S - сумма больших и меньших квадратов отклонений из подвыборок}
$$

Шаг 6.

Сравниваются значения $F_{расч}$ и $F_{крит}(\alpha,\nu_1=\nu_2=k-m-1)$ => гетероскедастичность
    """))


def task_72():
    display(Markdown(r"""
Для построения интервальных оценок параметров предусматривается построение t-статистик.

Для их формирования используются вспомогательные случайные величины.

$$ 1) V=\frac{e^Te}{\sigma^2}$$

$$ 2) Z_{\beta_j} = \frac{\beta_j - \hat{\beta_j}}{\sigma_j}  N(0;1) - $$стандартная нормальная случайная величина




Тогда $t_{\beta_j} = \frac{\beta_j - \hat{\beta_j}}{S_{\beta_j}}$ используется для проверки статистической значимости оценок параметров множественной регрессии.   

При справедливости гипотезы $ H_0: \beta_j = 0 $ вычисляется статистика вида $ \vert t \vert = \vert \frac{\hat{\beta_j}}{S_{\beta_J}} \vert $, имеющая распределение Стьюдента (n - объём выборки, k – число параметров модели).  

Вычисленное значение сравнивается с критическим, и если критическое значение меньше наблюдаемого, то нулевая гипотеза отвергается и коэффициент признаётся статистически значимым. Если наоборот (коэффициент признаётся незначимым), то регрессор $x_i$ рекомендует исключить из уравнения регрессии. Т.к. он не оказывает существенного влияния на эндогенную переменную.

    """))


def task_73():
    display(Markdown(r"""
В уравнение регрессии включена лишняя (незначимая) переменная. В связи с этим оценки параметров останутся несмещенными, но потеряют свою эффективность (их точность не является максимально возможной).

Последствия этой ошибки могут привести к выводу о неадекватности модели. При больших значениях незначимой объясняющей переменной ошибка оценки становится составной частью ошибки прогноза, увеличивая тем самым его неточность.

Симптом данной ошибки спецификации заключается в том, что оценка лишнего регрессора по абсолютной величине находится на уровне своей стандартной ошибки, вследствии чего их соотношение (которое можно использовать в качестве статистики критерия гипотезы о незначимости выбранного регрессора) принимает по абсолютной величине небольшие значения.


Таким образом можно проверить каждую из выбранных переменных (сравнивая значение статистик Стьюдента с критическим значением), и сделать вывод о включении или исключении конкретной переменной из рассмотрения.
    """))


def task_74():
    display(Markdown(r"""
Измерить тесноту связи, а также количественно оценить ее можно посредством корреялционного анализа. Парный коэффециент корреляции характеризует направление и степень тесноты связи между двумя признаками.
$r_{xy} = \frac{\overline{xy} - \overline{x} * \overline {y}}{\sigma_x * \sigma_y}$

Тк корряляция принимает значения от [-1:1], то знак показывает направление связи, а значение - тесноту. 

    0 - связь отстуствует
    0-0.3 - очень слабая 
    0.3-0.5 - слабая
    0.5-0.7 - умеренная
    0.7-1 - сильная
    1 - функциональная
    
В ходе корреляционного анализа составляется корреляционная матрица, в которой отображены значения корреляции между элементами. Матрица - симметрична относительно главной диагонали, а сама главная диагональ состоит из единиц. По корреляционной матрице можно выявить мультиколлинеарность. Для парной регрессии составляется матрица 2x2, где по главной диагонали находится 1.
    """))


def task_75():
    display(Markdown(r"""
Эконометрическое моделирование подразумевает оценивание параметров  использованием статистических методов. Статистическая процедура оценивания параметров - задача, цель которой является оценить неизвестное значение параметра наилучшим (в каком-либо смысле) образом. К линейным статистическим процедурам относятся МНК и иногда метод максимального правдоподобия.

Состоятельная оценка – оценка, сходящаяся по вероятности к оцениваемому параметру

Несмещенная оценка – это оценка параметра, математическое ожидание которой равно значению оцениваемого параметра 

Эффективная оценка – это несмещенная оценка, имеющая наименьшую дисперсию из всех возможных несмещенных оценок данного параметра
    """))


def task_76():
    display(Markdown(r"""
Случайным вектором или многомерной случайной переменной называется упорядоченный набор случайных величин, который обозначается как $\overline{X} = (x_1, x_2, ..., x_n)$.
Функцией распределения многомерной случайной переменной $\overline{X} = (x_1, x_2, ..., x_n)$ называется функция $F = (q_1, q_2, ..., q_n)$, которая определяет вероятность появления совместного события $(x_1 < q_1, x_2 < q_2, ..., x_n < q_n)$, т.е.
$F = (x_1, x_2, ..., x_n) = P(x_1 < q_1, x_2 < q_2, ..., x_n < q_n)$
Совместной плотностью распределения вероятностей многомерной случайной переменной есть смешанная производная от функции распределения
$P_{\overline{X}} = (x_1, x_2, ..., x_n) = \frac{\partial^n F(x_1, x_2, ..., x_n)}{\partial x_1, \partial x_2, ..., \partial x_n}$

Условные количественные характеристики случайных переменных (x, у) представляют собой функции соответственно у uх. Любую случайную переменную всегда можно представить в виде суммы двух слагаемых: константы, например математического ожидания этой переменной, и случайного возмущения с нулевым средним значением:
$x=m_x + u$, где $m_u=0$
    """))


def task_77():
    display(Markdown(r"""
В классическом понимании в регрессионной модели рассматриваются только количественные переменные. Однако для повышения качества модели возникает необходимость применения качественных признаков. 

Фиктивные (искусственные) переменные – это переменные, которые количественным образом описывают качественные признаки. В эконометрических моделях обычно используются фиктивные переменные бинарного типа «0-1», которые свидетельствуют о наличии или отсуствии признака

Dt = $\left\{\begin{matrix}
 0 & отсуствие 
\\\ 1 & наличие
\\\end{matrix}\right.$

Если качественная переменная имеет k альтернативных значений, то при моделировании используются только (k-1) фиктивная переменная. 

Параметр при фиктивной переменной показывает, что целевая переменная Y изменяется на величину параметра при истинном значении фиктивной переменной. 
    """))


def task_78():
    display(Markdown(r"""
При малом объеме выборки, что наиболее
характерно для эконометрических
исследований, для оценки гетеро-
скедастичности может использоваться метод
Голфелда-Куандта, разработанный в 1965г.
Голфелд и Куандт рассмотрели одно-
факторную линейную модель, для которой
дисперсия остатков возрастает
пропорционально квадрату фактора.

Алгоритм:
1. Все наблюдения n упорядочиваются по величине x.
2. Вся упорядоченная выборка разбивается на 3 части: k,
(n-k), k.
3. Оцениваются регрессии для первых k наблюдений и для
третьих k наблюдений.
4. По каждой подвыборке рассчитывается сумма квадратов
отклонений:
$$S_1 = \sum_{i=1}^{k} e_i ^2$$
$$S_3 = \sum_{i=n-k+1}^{n} e_i ^2$$
5. Рассчитывается $F_{расч} ^{i=1}$ - статистика Фишера: 
$$F_{расч} = \frac{S_3}{S_1}$$ 
6. Сравнение $F_{расч}$ и $F_{кр}(α, υ_1= υ_2=k-m-1)$, => гетероскедастичность.
    """))


def task_79():
    display(Markdown(r"""
**Регрессионным анализом** называется определение аналитического выражения связи между исследуемыми переменными, в котором изменение результативной переменной происходит под влиянием факторной переменной.

Прогнозировать значения эндогенной переменной можно лишь тогда, когда модель признана адекватной. Модель называется **адекватной**, если прогнозы значений эндогенной переменной согласуются с ее наблюденными значениями (они также являются значимыми, как и все построенное уравнение (случайные возмущения в уравнениях для каждого наблюдения удовлетворяют всем предпосылкам теоремы Гаусса-Маркова)). Таким образом, прогнозы по оцененной модели эндогенной переменной используются и в процедуре проверки адекватности данной модели.

Прогноз называется точечным оптимальным прогнозом, т.к. результатом прогноза является конкретное значение (точка, число) величины y.
    """))


def task_80():
    display(Markdown(r"""
Типы нелинейности эконометрических моделей:

Гиперболическая модель: $y=\beta_0+\frac{\beta_1}{x}+\epsilon$
        
Логарифмическая модель: $y=\beta_0+\beta_1ln(x)+\epsilon$

Степенная модель: $y=\beta_0\cdot x^{\beta_1}\epsilon$

Показательная: $y=\beta_0\cdot e^{\beta_1\cdot x}\epsilon$

Классы нелинейных эконометрических моделей:

$\cdot$ нелинейные, относительно объясняющих переменных, но линейные по оцениваемым параметрам (гиперболическая)

$\cdot$ нелинейные, относительно объясняющих переменных и нелинейные по оцениваемым параметрам (степенная, показательная)

| Вид зависимости | Ограничение | Замена переменных |   | Обратная замена |   |
|-----------------|-------------|-------------------|---|-----------------|---|
|Гиперболическая|$x\neq0$| $g=y$ | $t=\frac{1}{x}$|$a=c$|$b=d$|
|Логарифмическая|$x>0$| $g=y$ | $t=ln(x)$ |$a=c$|$b=d$|
|Степенная       |$x,y>0$| $g=ln(y)$ | $t=ln(x)$|$a=e^c$|$b=d$|
|Показательная|$y>0$| $g=ln(y)$ |$t=x$|$a=e^c$|$b=d$|

Способы включения случайных возмущений $\epsilon_i$:

$\cdot$ Мультиплиĸативные: $y_t=\alpha\cdot x^{\beta}_t\cdot\epsilon_i$

$\cdot$ Аддитивные: $y_t=\alpha\cdot x^{\beta}_t+\epsilon_i$

$\cdot$ $y_t=\alpha \cdot x^{\beta}_t\cdot e^{\epsilon_i}$
    """))


def task_81():
    display(Markdown(r"""
Типы нелинейности эконометрических моделей:

Гиперболическая модель: $y=\beta_0+\frac{\beta_1}{x}+\epsilon$
        
Логарифмическая модель: $y=\beta_0+\beta_1ln(x)+\epsilon$

Степенная модель: $y=\beta_0\cdot x^{\beta_1}\epsilon$

Показательная: $y=\beta_0\cdot e^{\beta_1\cdot x}\epsilon$

Классы нелинейных эконометрических моделей:

$\cdot$ нелинейные, относительно объясняющих переменных, но линейные по оцениваемым параметрам (гиперболическая)

$\cdot$ нелинейные, относительно объясняющих переменных и нелинейные по оцениваемым параметрам (степенная, показательная)

Функция Кобба-Дугласа:

$$
Q = \alpha\cdot K^{\beta}\cdot L^{\gamma}
$$

Q - объём выпущенной продукции (в стоимостном или натуральном выражении)

K - объём основного капитала или основных фондов

L – объём трудовых ресурсов или трудовых затрат

Линеаризировать эту функцию можно путем логарифмирования каждого из компонентов:

$$
ln(y)=ln(\alpha)+\beta ln(K) + \gamma ln(L)
$$
    """))


def task_82():
    display(Markdown(r"""
Графический:

Для квадратов отклонений (остатков) построить график зависимости случайных остатков $\varepsilon_i$ от факторов $x_i$. Если точки не имеют направленности, то это означает, что гетероскедастичности нет, то есть присутствует гомоскедастичность. Если расположение остатков на графике имеет определенную направленность, то это означает, что гетероскедастичность есть. Модель неадекватна.

Тест Бреуша-Пагана
H0: остатки гомоскедастичности $\gamma_2=\gamma_p=0$
1. Коэффициенты регрессии определяются МНК
2. Находим дисперсию ошибки модели $\widehat{\sigma}^2 = \frac{1}{n}RSS$
3. Вычисляем стандартизированные остатки 
$\frac{\varepsilon^2} {\widehat{\sigma}^2}$
4. Строится дополнительная регрессия квадратов стандартизированных ошибок на исходные наблюдаемые переменные:
$\widehat{\varepsilon}^2_t = \gamma_1 + \gamma_2z_{2t}+...+\gamma_pz_{pt}+ \eta_t$
5. LM=n*$R^2$,  где $R^{2}$ — коэффициент детерминации построенной на предыдущем шаге регрессии

При справедливости нулевой гипотезы о гомоскедастичности остатков статистика критерия имеет распределение хи-квадрат с p-1 степенями свободы.
    """))


def task_83():
    display(Markdown(r"""
Гетероскедастичность – непостоянство дисперсии отклонений. 

При малом объеме выборки, что наиболее
характерно для эконометрических
исследований, для оценки гетеро-
скедастичности может использоваться метод
Голфелда-Куандта, разработанный в 1965г.
Голфелд и Куандт рассмотрели одно-
факторную линейную модель, для которой
дисперсия остатков возрастает
пропорционально квадрату фактора.

Алгоритм:
1. Все наблюдения n упорядочиваются по величине x.
2. Вся упорядоченная выборка разбивается на 3 части: k,
(n-k), k.
3. Оцениваются регрессии для первых k наблюдений и для
третьих k наблюдений.
4. По каждой подвыборке рассчитывается сумма квадратов
отклонений:
$$S_1 = \sum_{i=1}^{k} e_i ^2$$
$$S_3 = \sum_{i=n-k+1}^{n} e_i ^2$$
5. Рассчитывается $F_{расч} ^{i=1}$ - статистика Фишера: 
$$F_{расч} = \frac{S_3}{S_1}$$ 
6. Сравнение $F_{расч}$ и $F_{кр}(α, υ_1= υ_2=k-m-1)$, => гетероскедастичность.
    """))


def task_84():
    display(Markdown(r"""
Авторегрессионная схема первого порядка - метод устранения автокорреляции первого порядка между соседними членами ряда остатков в линейных моделях регрессии либо моделях регрессии, сводящихся к линейному виду.

Поскольку величина коэффициента автокорреляции неизвестна, то в качестве его оценки используется выборочный автокорреляционный коэффициент остатков первого порядка $p_1$:

$$
p_1 = \frac{\sum^T_{t=2}e_te_{t-1}}{\sum^T_{t=2}e^2_t}
$$

Имея (преположим) модель парной регрессии:

$$
y_t = \beta_0+\beta_1x_t+e_t
$$

Соответсвующая модель регрессии с учетом процесса автокорреляции остатков первого порядка:

$$
y_t=\beta_0+\beta_1x_t+pE_{t-1}+V_t
$$

$V_t$ - независимые, одинаково распределенные случайные величины с нулевым математическим ожиданием

Для получения регрессионной модели с учетом автокорреляции первого порядка необходимо вычесть из последнего уравнения тоже уравнение в момент времени $t-1$ домноженное на p. В итоге получится:

$$
y_t-py_{t-1} = \beta_0(1-p)+\beta_1(x_t-px_{t-1}) + V_t
$$
    """))


def task_85():
    display(Markdown(r"""
Гетероскедастичность - непостоянство дисперсий отклонений.

Причины:

$\cdot$ Эффект масштаба пространсвенно временных рядов

$\cdot$ Эффект запаздывания данных

$\cdot$ Эффект запаздывания данных

Последствие:

$\cdot$ Оценки перестают быть эффективными

При подтверждении наличия гетероскедастичности необходимо преобразовать модель с целью смягчения ее влияния. Для этого предлагают применять МВНК (метод взвешенных наименьших квадратов).

Этапы МВНК:

$\cdot$ Каждое из наблюдений $y_i$, $x_i$ делят на известную величину $\sigma_i$ (дисперсия случайных отклонений известна - $\sigma_i=\sqrt{\sigma_j^2}$; дисперсия случайнх отклонений неизвестна - $\sigma_i = \sqrt{x_i}$ при пропорциональности дисперсии и $x_i$ или $\sigma_i = x_i$ при пропорциональности дисперсии и $x_i^2$)

$\cdot$ По МНК для преобразованиязначений строится уравнение регрессии с гарантированным качеством оценок (остатки гомоскедастичны)
    """))


def task_86():
    display(Markdown(r"""
Общий вид уравнений для отображения зависимостей между показателями:

$$
y=f(X)+\epsilon
$$

f(X) - математическая функция, определяющая закономерность между эндогенной и предопределенными переменными

$\epsilon$ - случайная величина, учитывающая влияние неучтенных факторов и индивидуальные особенности конкретного объекта (случайное возмущение)

Предпосылки теоремы Гаусса-Маркова:
    
1. Математическое ожидание: $M(\epsilon_i)=0$ - для всех наблюдений

2. Дисперсия случайных отклонений $\epsilon_i$ постоянна для любых наблюдений

$$
Var(\epsilon_i) = Var(\epsilon_i) = \sigma^2 \approx const - \text{постоянство дисперсий}
$$

3. Случайные отелонения являются независимыми друг от друга.

4. Случайные отклонения должны быть независимы от объясняющих переменных (экзогенных).

5. Модель является линейной относительно параметров.

6. Отсутсвие мультиколлинеарности.

7. Ошибки должны иметь нормальное распределение.
    """))


def task_87():
    display(Markdown(r"""
Для интервального прогноза используется следующее отношение:

$$
t = \frac{\widetilde{y}_0-y_0}{S\widetilde{y}_0} - \text{нормированная ошибка прогноза}
$$

Если случайный остаток в модели не имеет автокорреляции и нормально распределен, то дробь обладает законом распределения Стьюдента с числом степеней свободы = $n-k-1$

Данное обстоятельство позволяет построить замкнутый промежуток с границами именуемый доверительным интервалом, который накрывает прогнозируемое значение с принятой доверительной вероятностью ($\alpha$)

Правило проверки адекватности оцененной модели:

1) Результаты наблюдений следует разделить на два класса. В первый класс (обучающую выборку) включить основной объем результатов наблюдений. Оставшиеся результаты наблюдений составят контролирующую выборку.

2) По обучающей выборке оценить модель.

3) Задаться доверительной вероятностью $\alpha$ и по значения регрессоров, входящих в контролирующую выборку, построить доверительные интервалы для сооветствующих этим регрессорам значени эндогенной переменной модели

4) Проверить, попадают ли значения эндогенной переменной из контролирующей выборки в соответствующие доверительные интервалы. Если да, то признать оцененную модель адекватной; если же нет, то оцененная модель не может быть признана адекватной и подлежит доработке
    """))


def task_88():
    display(Markdown(r"""
В методе необходимо подсчитать количество положительных и отрицательных отклонений, а также выделить в ряду отклонений подрядов последовательных отношений имеющих один знак. Количество таких подрядов обозначить "к". При использовании метода рядов последовательно определяются знаки отклонений е. Ряд определяется как непрерывная последовательность одинаковых знаков. Количество знаков в ряду называется длиной ряда.  


Для более детального анализа предлагается следующая процедура. 
Пусть:  
n - объем выборки; ·        
$n_1$ - общее количество знаков "+" при n наблюдениях (количество положительных отклонений е);         
$n_2$ - общее количество знаков " - " при n наблюдениях (количество отрицательных отклонений е); ·        
к - количество рядов.  

При достаточно большом числе наблюдений $n_1 \geq 10 $ и $n_2 \geq 10$ случайная величина k имеет асимптотические нормальные распределения  

$$ M(k) = \frac{2n_1n_2}{n_1+n_2} +1$$  

$$ D(k) = \frac{2n_1n_2(2n_1n_2-n_1-n_2)}{(n_1+n_2)^2(n_1+n_2-1)}$$  


Если $$ M(k) - \mu_{\frac{\alpha}{2}}D(K)  \leq k  \leq M(k) + \mu_{\frac{\alpha}{2}}D(K) $$  

то гипотеза об отсутсвие автокорреляции отклоняется


    """))


def task_89():
    display(Markdown(r"""

Автокорреляция случайного возмущения-невыполнение 3 Гаусса-Маркова о независимости случайных переменных в уравнениях наблюдений

Автоковариационная матрица вектора случайных возмущений при наличии автокорреляции имеет структуру:

$$\Omega = 
 \begin{pmatrix}
  \sigma^2 & \Omega_{1,2} & \cdots & \Omega_{1,n} \\
  \Omega_{2,1} & \sigma^2 & \cdots & \Omega_{2,n} \\     
  \vdots  & \vdots  & \sigma^2& \vdots  \\
  \Omega_{n,1} &  \cdots& \cdots & \sigma^2 
 \end{pmatrix}$$


$$\Omega_{t,s}=Cov(E_t,E_s)\neq 0$$
Причинами автокорреляции являются: 

ошибки спецификации модели (пропуск важной объясняющей переменной, использование ошибочной функциональной зависимости между переменными);

ошибки измерений; 

характер наблюдений (например, данные временных рядов). 

Последствия: оценки перестают быть эффективными -> увеличивается дисперсия оценок, которая снижает вероятность получения масимально точных оценок. 

Выводы на основе t и F будут ненадежными.

Отметим два важных свойства коэффициента автокорреляции. Во – первых, он строится по аналогии с линейным коэффициентом корреляции и, таким образом, характеризует тесноту только линейной связи текущего и предыдущего уровней связи. Поэтому по коэффициенту автокорреляции можно судить о наличии линейной (или близкой к линейной) тенденции. Для некоторых временных рядов, имеющих сильную нелинейную тенденцию (например, параболу второго порядка или экспоненту), коэффициент автокорреляции уровней исходного ряда может приближаться к нулю.

Во – вторых, по знаку коэффициента автокорреляции нельзя делать вывод о возрастающей или убывающей тенденции в уровнях ряда. Большинство временных рядов экономических данных содержит положительную автокорреляцию уровней, однако при этом они могут иметь убывающую тенденцию.

    """))


def task_90():
    display(Markdown(r"""
Соотношения между социально-экономическими явлениями и процессами далеко не всегда можно выразить линейными функциями, так как при этом могут возникать неоправданно большие ошибки. В таких случаях используют нелинейную регрессию. Таким образом, если между экономическими явлениями существуют нелинейные соотношения, то они выражаются с помощью соответствующих нелинейных функций  


Различают два класса нелинейных регрессий:

• регрессии, нелинейные относительно включенных в анализ объясняющих переменных, но линейные по оцениваемым параметрам:

1) полином k степеней,

$ y_i = b_0 + b_1*x_i + b_2*x_i^2 + b_3*x_i^3 + e_i $

2) гипербола

$ y_i = b_0 + \frac{b_1}{x_i}+e_i $  



• регрессии, нелинейные по оцениваемым параметрам:
1) степенная 

$ y_i = b_0 * x_i^{b_1} $  

2) показательная   

$ y_i = b_0 * b_i^{x_i} $ 

3) экспоненциальная  

$ y_i = e^{b_0+b_1*x_i}$   


Линеаризация применяется для построения нелинейных зависимостей при использовании линейной регрессионной модели. 

Методы линеаризации

1) Метод логарифмирования — применяется к степенным функциям;  
2) Метод обратного преобразования — для дробных функций;  
3) Комплексный метод — для дробных и степенных функций.  
  
  
| Вид зависимости | Ограничение | Замена переменных |   | Обратная замена |   |
|-----------------|-------------|-------------------|---|-----------------|---|
|Гиперболическая|$x\neq0$| $g=y$ | $t=\frac{1}{x}$|$a=c$|$b=d$|
|Логарифмическая|$x>0$| $g=y$ | $t=ln(x)$ |$a=c$|$b=d$|
|Степенная       |$x,y>0$| $g=ln(y)$ | $t=ln(x)$|$a=e^c$|$b=d$|
|Показательная|$y>0$| $g=ln(y)$ |$t=x$|$a=e^c$|$b=d$|

Для оценки параметров данных регрессионных моделей используют:

$\cdot$ МНК

$\cdot$ Метод моментов

$\cdot$ Метод максимального правдоподобия

    """))


def task_91():
    display(Markdown(r"""
**Автокорреляция** (последовательная корреляция) остатков определяется как корреляция между соседними значениями случайных отклонений во времени (временные ряды) или в пространстве (перекрестные данные). Она обычно встречается во временных рядах и очень редко – в пространственных данных.


Применение МНК к данным, имеющим автокорреляцию в остатках, приводит к таким последствиям:  

1) Оценки параметров, оставаясь линейными и несмещенными, перестают быть эффективными. Они перестают быть наилучшими линейными несмещенными оценками.

2) Дисперсии оценок являются смещенными. Часто дисперсии, вычисляемые по стандартным формулам, являются заниженными, что влечет за собой увеличение t-статистик. Это может привести к признанию статистически значимыми факторов, которые в действительности таковыми не являются.

3) Оценка дисперсии регрессии является смещенной оценкой истинного значения  , во многих случаях занижая его.

4) Выводы по t- и F-статистикам, возможно, будут неверными, что ухудшает прогнозные качества модели.

**ОМНК**

Обобщенный метод наименьших квадратов применяется к преобразованным данным и позволяет получать оценки, которые обладают не только свойством несмещенности, но и имеют меньшие выборочные дисперсии.  

Суть метода заключается в том, что подбираются коэффициенты $К_i$ такие, что  

$$ \sigma^2_{ei} =  \sigma^2 * K_i $$  

где:

$\sigma^2_{ei}$ - дисперсия ошибки при конкретном i-ом значении фактора  
$\sigma^2$ - постоянная дисперсия ошибки при соблюдении предпосылки о гомоскедастичности остатков;  
$ K_i $ - коэффициент пропорциональности, меняющийся с изменением величины фактора.

Уравнение парной регресси при этом принимает вид  


$$ \frac{y_i}{\sqrt{K_i}} = \frac{b_0}{\sqrt{K_i}} + \frac{b_1x_i}{\sqrt{K_i}} + e_i$$  


По отношению к обычной регрессии уравнение с новыми, преобразованными переменными представляют собой взвешенную регрессию, в которой переменные y и x взяты с весами $\frac{1}{\sqrt{K_i}}$ 

Аналогичный подход применяют и для множественной регрессии

1) Переход к относительным величинам существенно снижает вариацию фактора и соответственно уменьшает дисперсию ошибки.  
2) Метод обеспечивает максимальную информативность и минимальное искажение геометрической структуры исходных данных. Вычисление главных компонент сводится к вычислению собственных векторов и собственных значений ковариационной матрицы исходных данных.  

    """))


def get_task_by_id(id: int) -> Union[None, str]:
    all_tasks = load_all_tasks()
    for task in all_tasks:
        if task.id == id:
            if (task.unit == 'a'):
                eval("task_"+str(id)+"()")
            else:
                filehandle=pathlib.Path(pathlib.Path(os.path.dirname(os.path.abspath(__file__)), "tasks.pdf"))
                doc = fitz.open(filehandle)
                for i in range(task.task_solution[0], task.task_solution[0] + 1):
                    page = doc.load_page(task.task_solution)
                    pix = page.get_pixmap()
                    output = "outfile.png"
                    pix.save(output)
                    display(Image(output))
                doc.close()


get_task_by_id(44)