import absl.app
from transformers import AutoTokenizer
from EasyDel import configs
from EasyDel.serve import JAXServer
import EasyDel
from absl import flags

FLAGS = flags.FLAGS

flags.DEFINE_string(
    name='ckpt_path',
    required=True,
    help='path to model weights for example (ckpt/llama_easydel_format)',
    default=None
)
flags.DEFINE_string(
    name='model_type',
    default='7b',
    help='which model type of llama 1 to train example [13b , 7b , 3b ,...] (default is 7b model)'
)

flags.DEFINE_bool(
    name='use_flash_attention',
    default=False,
    help='use_flash_attention or no'
)

flags.DEFINE_bool(
    name='use_sacn_mlp',
    default=False,
    help='use_sacn_mlp or no'
)

flags.DEFINE_integer(
    name='max_sequence_length',
    default=2048,
    help='max sequence length for model to train'
)

flags.DEFINE_string(
    name="rotary_type",
    default='complex',
    help='what kind of implementation of rotary embedding to be used for model (available are lm2, open, complex) '
)

flags.DEFINE_string(
    name="ip",
    default='0.0.0.0',
    help='ip to run server on (by default on localhost)'
)

flags.DEFINE_integer(
    name="port",
    default=2059,
    help='port to run server on (by default on 2059)'
)

flags.DEFINE_integer(
    name="max_length",
    default=2048,
    help='max length of token for model'
)

flags.DEFINE_integer(
    name="max_new_tokens",
    default=2048,
    help='number of max new tokens to be generated by model'
)

flags.DEFINE_integer(
    name="max_stream_tokens",
    default=32,
    help='number of tokens being streamed and generated at time '
)

flags.DEFINE_integer(
    name="seed",
    default=552,
    help='seed'
)

flags.DEFINE_integer(
    name="top_k",
    default=50,
    help='top_k to generate outputs'
)
flags.DEFINE_bool(
    name='use_prefix_tokenizer',
    default=True,
    help='use_prefix_tokenizer'
)
flags.DEFINE_string(
    name="dtype",
    default='fp16',
    help='model and parameters data type'
)

flags.DEFINE_float(
    name='temperature',
    default=0.1,
    help='model temperature to generate outputs'
)

flags.DEFINE_float(
    name='top_p',
    default=0.95,
    help='model top_p to generate outputs'
)


def main(argv):
    conf = EasyDel.configs.configs.llama_configs[FLAGS.model_type]
    config = EasyDel.LlamaConfig(**conf, rotary_type=FLAGS.rotary_type)
    config.use_flash_attention = FLAGS.use_flash_attention
    config.use_sacn_mlp = FLAGS.use_sacn_mlp
    config.rope_scaling = None
    config.max_sequence_length = FLAGS.max_sequence_length
    config.use_pjit_attention_force = False
    model = EasyDel.FlaxLlamaForCausalLM(config, _do_init=False)
    tokenizer = AutoTokenizer.from_pretrained('erfanzar/JaxLLama')

    # This config works fine with all the models supported in EasyDel
    config_server = {
        "host": FLAGS.ip,
        "port": FLAGS.port,
        "instruct_format": '### SYSTEM:\n{system}\n### INSTRUCT:\n{instruct}\n### ASSISTANT:\n',
        "chat_format": '<|prompter|>{prompt}</s><|assistant|>{assistant}</s>',
        "batch_size": 1,
        "system_prefix": '',
        "system": '',
        "prompt_prefix_instruct": '',
        "prompt_postfix_instruct": '',
        "prompt_prefix_chat": '<|prompter|>',
        "prompt_postfix_chat": '</s><|assistant|>',
        "is_instruct": False,
        "chat_prefix": '',
        "contains_auto_format": True,
        "max_length": FLAGS.max_length,
        "max_new_tokens": FLAGS.max_new_tokens,
        "max_stream_tokens": FLAGS.max_stream_tokens,
        "temperature": FLAGS.temperature,
        "top_p": FLAGS.top_p,
        "top_k": FLAGS.top_k,
        "logging": True,
        "mesh_axes_names": ('dp', 'fsdp', 'mp'),
        "mesh_axes_shape": (1, -1, 1),
        "dtype": FLAGS.dtype,
        "seed": FLAGS.seed,
        "use_prefix_tokenizer": FLAGS.use_prefix_tokenizer
    }
    server = JAXServer.load(
        path=FLAGS.ckpt_path,
        model=model,
        tokenizer=tokenizer,
        config_model=config,
        add_params_field=True,
        config=config_server,
        init_shape=(1, 1)
    )
    server.fire()
    # Done you have hosted your LLama model :)
    # You Can access the instruct gradio interface in ip:port/gradio_instruct
    # You Can access the chat gradio interface in ip:port/gradio_chat
    # FastAPI Instruct access points ['/chat','/instruct','/status']


if __name__ == "__main__":
    absl.app.run(main)
