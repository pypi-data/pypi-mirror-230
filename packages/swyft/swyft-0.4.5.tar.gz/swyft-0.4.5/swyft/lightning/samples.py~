from typing import (
    Callable,
    Dict,
    Hashable,
    Optional,
    Sequence,
    Tuple,
    Type,
    TypeVar,
    Union,
)
import numpy as np
import torch
import pytorch_lightning as pl
import swyft


######################
# Datasets and loaders
######################


class SwyftDataModule(pl.LightningDataModule):
    """DataModule to handle simulated data.

    Args:
        data: Simulation data
        lenghts: List of number of samples used for [training, validation, testing].
        fractions: Fraction of samples used for [training, validation, testing].
        batch_size: Minibatch size.
        num_workers: Number of workers for dataloader.
        shuffle: Shuffle training data.

    Returns:
        pytorch_lightning.LightningDataModule
    """

    def __init__(
        self,
        data,
        lengths: Union[Sequence[int], None] = None,
        fractions: Union[Sequence[float], None] = None,
        batch_size: int = 32,
        num_workers: int = 0,
        shuffle: bool = False,
    ):
        super().__init__()
        self.data = data
        if lengths is not None and fractions is None:
            self.lengths = lengths
        elif lengths is None and fractions is not None:
            self.lengths = self._get_lengths(fractions, len(data))
        else:
            raise ValueError("Either lenghts or fraction must be set, but not both.")
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.shuffle = shuffle

    @staticmethod
    def _get_lengths(fractions, N):
        fractions = np.array(fractions)
        fractions /= sum(fractions)
        mu = N * fractions
        n = np.floor(mu)
        n[0] += N - sum(n)
        return [int(v) for v in n]

    def setup(self, stage: str):
        if isinstance(self.data, Samples):
            dataset = self.data.get_dataset()
            splits = torch.utils.data.random_split(dataset, self.lengths)
            self.dataset_train, self.dataset_val, self.dataset_test = splits
        elif isinstance(self.data, swyft.ZarrStore):
            idxr1 = (0, self.lengths[1])
            idxr2 = (self.lengths[1], self.lengths[1] + self.lengths[2])
            idxr3 = (self.lengths[1] + self.lengths[2], len(self.data))
            self.dataset_train = self.data.get_dataset(
                idx_range=idxr1, on_after_load_sample=None
            )
            self.dataset_val = self.data.get_dataset(
                idx_range=idxr2, on_after_load_sample=None
            )
            self.dataset_test = self.data.get_dataset(
                idx_range=idxr3, on_after_load_sample=None
            )
        else:
            raise ValueError

    def train_dataloader(self):
        dataloader = torch.utils.data.DataLoader(
            self.dataset_train,
            batch_size=self.batch_size,
            shuffle=self.shuffle,
            num_workers=self.num_workers,
        )
        return dataloader

    def val_dataloader(self):
        dataloader = torch.utils.data.DataLoader(
            self.dataset_val,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
        )
        return dataloader

    def test_dataloader(self):
        dataloader = torch.utils.data.DataLoader(
            self.dataset_test,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
        )
        return dataloader


class SamplesDataset(torch.utils.data.Dataset):
    """Simple torch dataset based on Samples."""

    def __init__(self, sample_store, on_after_load_sample=None):
        self._dataset = sample_store
        self._on_after_load_sample = on_after_load_sample

    def __len__(self):
        return len(self._dataset[list(self._dataset.keys())[0]])

    def __getitem__(self, i):
        d = {k: v[i] for k, v in self._dataset.items()}
        if self._on_after_load_sample is not None:
            d = self._on_after_load_sample(d)
        return d


class RepeatDatasetWrapper(torch.utils.data.Dataset):
    def __init__(self, dataset, repeat):
        self._dataset = dataset
        self._repeat = repeat

    def __len__(self):
        return len(self._dataset) * self._repeat

    def __getitem__(self, i):
        return self._dataset[i // self._repeat]
